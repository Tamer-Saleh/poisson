So that we're all on the same page, let's start from the beginning. The goal of this project is to utilize new sources of satellite imagery to identify small fishing vessels and study their behavior. Last summer I began working on an approach to automatically identify vessels in satellite imagery using deep learning. Although this model achieved a reasonable level of performance, it appeared to underperform in several edge cases, and because I had used off-the-shelf architectures with pretrained weights, it was nearly impossible to verify the effect of each component on the model's overall performance.

After restarting my work this fall, I therefore chose to develop a new vessel detection model from the ground up, component-by-component. In this way I would be able to make sure that each component in the algorithm is functioning correctly. 

Although there is a lot of glue code supporting the functioning of the model, the algorithm may be broadly divided into four parts:
- A preprocessing part which transforms the training, validation and test data into the format required by the prediction algorithm.
- A "backbone", a convolutional neural network which takes preprocessed images as input and derives features relevant to the task of vessel detection from the images.
- An object detection "head", a CNN which takes the features from the backbone as input and from these generates a prediction mask for the preprocessed images. This prediction mask identifies what locations in the image the model object detection algorithm believes to contain vessels, if any.
- A postprocessing part which transforms the outputs of the object detection algorithm into a format suitable for human use and stores them.

Rather than training both the backbone and the object detection algorithm together, as a single neural network, I chose to train each independently as this allowed me to verify their performance separately and make sure that each component was functioning appropriately.

To select an appropriate architecture for the backbone I trained several alternative models for a short period of time and compared their performance on a small test set. Based on the results, I chose to use the so-called "Inception" architecture. This model achieved a similar precision, recall, and accuracy score to the other models while requiring less computational resources to train.

I then trained this model from scratch using satellite images and labels from two openly-available training datasets. Although each dataset included satellite images obtained from multiple classes of satellites,there were slight differences in the coverage of each. The smaller dataset contained more images including smaller vessels, but because this dataset was approximately 33 times smaller than the larger dataset, I chose to combine them into a single training dataset. This dataset contained 193,000 samples. From this set I created a validation by selecting 1,930 samples. I chose these samples so that the class distributions in the validation set and the training set were equal, each containing approximately 91.98 percent negative  samples, and 8.02 percent positive samples. Although the size of training images varied, the most common size was 768 by 768 (with three channels).  The InceptionV3 architecture, however, expects input images of size 299 by 299 by 3. I therefore down-sampled the training image to a standard size of 299 by 299 by 3, with an interpolation of 2.0.

The model was trained for 79 complete epochs on the training dataset, seeing 15,182,640 images in total and running for 357.6 hours. The mean loss on the validation set consistently decreased from epochs 1 - 45, after which loss remained roughly constant, hovering around 0.06 (see Figure 3). The minimum loss recorded on the validation set was 0.05, encountered after both the 63 and 77th epochs. Mean accuracy on the validation set consistently increased for the first 50 epochs, after which it stabilized at approximately 98 percent (see Figure 4). The peak validation accuracy was 98.4 percent, recorded after the 72nd epoch.The validation dataset contains a disproportionate number of negative samples, and therefore a naïve model trained to label all samples as negative would achieve a validation accuracy of 91.98 percent. We see that the model's accuracy after the first epoch was 92.0 percent, barely above this threshold. Model performance consistently improved thereafter.

In practice, we intend to use this model with imagery obtained from Planet Labs, inc. These images are 24 by 7 kilometers with a spatial resolution of 3 meters per pixel. To create a test data set that accurately represented these images, I selected about 30 of these images, depicting the littoral waters of Peru from the last five years, and sliced them into 299 by 299 pixel tiles. I then selected 520 of these tiles and classified them according to the presence or absence of vessels within each image. Approximately 89.6% of these image tiles did not contain a vessel, which is likely much higher than the percentage of image tiles which actually contain an image in the real data. In the image tiles, I didn't label a large percentage because they were repetitive, and many contain no information at all (and are therefore simply black).  

The 520 samples were intentionally difficult, depicting complex weather conditions and variation in both land and sea surfacees. Nonetheless, the model generated 0 false positives, and correctly identified 12 of the 54 positive samples as containing a vessel. Although this implies a recall score of only about 27 precent, this result suggests that the number of true positives identified by the model in the target imagery will dominate the number of false positives, leading to data which accurately describes aggregate dynamics. Importantly, model performance is consistent across vessel sizes, even on the small end.

Once I had verified that the backbone model was properly trained, I began developing the object detection algorithm. I again compared several candidate algorithms, with both dense-prediction and sparse-prediction methods. Because of its computational efficiency I chose a YOLO architecture (which stands for "You Only Look Once") for the object detection algorithm. To train it, I combined this model with the backbone, which was of coure already trained, such that the activations of the final hidden layer of the backbone were fed as inputs into the object detection algorithm (and removed the final layer of the backbone). I fixed the parameters of the backbone, as they were already trained, thus allowing for the model to train only the object detection model. This vastly improves computational efficiency. 

Using training samples with bounding box ground truth labels, I then began training the object detection model. I started training on Monday, November 30, 2020, and I expect to be finished by sometime early next week.

Based on the performance of the backbone, I believe that the entire model will perform at a level suitable for real-world use. It is location and vessel-type invariant, and can be used on satellite imagery from anywhere in the world to identify vessels of all sizes, but particularly smaller vessels. This will allow use to quickly generate accurate data for large target areas and in places were almost no data existed before. After I complete the vessel detection model, we can begin generating data for the littoral waters of Peru. We may then begin looking for interesting patterns in this data and identify those which are worth further analysis. Once particularly promising use-case for this technology is in the identifying of dark-vessels, which do not have AIS/VMS equipment aboard or have turned it off. These vessels could only be identified previously by spotting ships and similar methods, which of course are much more limited in scope.