2020/11/5
- Model has completed 5 epochs since restart (43 total)
- Validation loss consistently decreasing but averaging higher than last validation loss
  before restart, likely due to the fact that a different validation set was chosen on 
  restart which the model finds more difficult. This resampling is an oversight on my
  part, and because of this leakage between validation and training, these validation
  metrics should only be used as a rough estimate of model performance.
- Because valid loss is decreasing consistently it appears that the model still has
  room for improvement.



2020/11/4
- Model completed 25 epochs since restart with smaller weight decay (38 overall)
- Validation loss continuing to drop
- Validation accuracy near 0.975 (moving average estimate - just a ballpark figure)

Because the virtual machine restarted during the 26th epoch, I restarted training.


2020/11/2
- Model completed 13 epochs since restart with smaller weight decay (27 epochs overall)
- Validation loss continuing to drop; currently around 0.085
- Change in validation loss per epoch continuing to decrease suggesting the model
  has almost converged

I intend to let the model continue to train until validation loss begins to consistently increase, or until the model has completed 44 epochs overall (30 on this training
session). The model is estimated to complete 44 epochs on or by 2020/11/5 at the current
pace.


2020/11/1
- Model completed 7 epochs with parameters
    - learning rate = 1e-4
    - beta 1 (Adam param.) = 0.9
    - beta 2 (Adam param.) = 0.999
    - weight decay = 1e-7 # Only change from previous training
- Validation loss is steady around 0.082 but decreasing overall (slowly)
- Accuracy on validation set after 20th iter. = 0.964

Next Steps:
    - Get some idea of a target performance level to determine when to finish training (e.g.):
        - Create baseline model (e.g. SVM) and train — compare performance to trained model 
	- Identify mislabeled samples
        - Change hyperparameters according to best practices recs.
    - Create dataset of classified PlanetScene images
    - Test performance on dataset
    - If the performance isn't acceptable then:
	- Add transformations for training data to make more similar to target data
	- Change hyperparams according to best practices
	- Retrain model with PlanetScene samples — follow best practices for situations
          where a model is fine-tuned on a much smaller dataset than the one on which
	  it was originally trained.
	- Repeat until model performs well enough
    - Think about how to use the classification model with the object detection model. 
      The previous object detection model used a ResNet50 architecture as its "backbone". 
      Presumably we can do the same thing with our classification model.	


2020/10/31
- Vessel classifier is training. 
- Model completed 14 epochs with parameters
    - learning rate = 1e-4
    - beta 1 (Adam param.) = 0.9
    - beta 2 (Adam param.) = 0.999
    - weight decay = 1e-5


2020/10/29
- Changed validation set to include only images blurred using Gaussian filter with
  radius = 2. The PlanetScene images are of a lower spatial res. than almost all of 
  the training samples and therefore the valid. set needs to include more blurred 
  images.
- Changed probability of Gaussian blur being applied to each training sample on ingest
  from 0.1 to 0.5 for the same reason.
- Started training