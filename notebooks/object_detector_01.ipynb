{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
    "from torchvision.models.detection.faster_rcnn import FasterRCNN, FastRCNNPredictor\n",
    "from torchvision.transforms import ToTensor, Compose, RandomHorizontalFlip,\\\n",
    "    RandomVerticalFlip, Resize, Normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image, ImageFile, ImageFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle2bbox(rle, shape):\n",
    "    '''\n",
    "    rle: run-length encoded image mask, as string\n",
    "    shape: (height, width) of image on which RLE was produced\n",
    "    Returns (x0, y0, x1, y1) tuple describing the bounding box of the rle mask\n",
    "    \n",
    "    Note on image vs np.array dimensions:\n",
    "    \n",
    "        np.array implies the `[y, x]` indexing order in terms of image dimensions,\n",
    "        so the variable on `shape[0]` is `y`, and the variable on the `shape[1]` is `x`,\n",
    "        hence the result would be correct (x0,y0,x1,y1) in terms of image dimensions\n",
    "        for RLE-encoded indices of np.array (which are produced by widely used kernels\n",
    "        and are used in most kaggle competitions datasets)\n",
    "    '''\n",
    "    \n",
    "    a = np.fromiter(rle.split(), dtype=np.uint)\n",
    "    a = a.reshape((-1, 2))  # an array of (start, length) pairs\n",
    "    a[:,0] -= 1  # `start` is 1-indexed\n",
    "    \n",
    "    y0 = a[:,0] % shape[0]\n",
    "    y1 = y0 + a[:,1]\n",
    "    if np.any(y1 > shape[0]):\n",
    "        # got `y` overrun, meaning that there are a pixels in mask on 0 and shape[0] position\n",
    "        y0 = 0\n",
    "        y1 = shape[0]\n",
    "    else:\n",
    "        y0 = np.min(y0)\n",
    "        y1 = np.max(y1)\n",
    "    \n",
    "    x0 = a[:,0] // shape[0]\n",
    "    x1 = (a[:,0] + a[:,1]) // shape[0]\n",
    "    x0 = np.min(x0)\n",
    "    x1 = np.max(x1)\n",
    "    \n",
    "    if x1 > shape[1]:\n",
    "        # just went out of the image dimensions\n",
    "        raise ValueError(\"invalid RLE or image dimensions: x1=%d > shape[1]=%d\" % (\n",
    "            x1, shape[1]\n",
    "        ))\n",
    "\n",
    "    return x0, y0, x1, y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(state_dict, num_classes):\n",
    "        inception = torchvision.models.inception_v3(pretrained=False, progress=False, \n",
    "                                                    num_classes=num_classes, aux_logits=False)\n",
    "        inception.load_state_dict(torch.load(state_dict))\n",
    "        modules = list(inception.children())[:-1]\n",
    "        backbone = nn.Sequential(*modules)\n",
    "\n",
    "        for layer in backbone:\n",
    "            for p in layer.parameters():\n",
    "                p.requires_grad = False # Freezes the backbone layers\n",
    "\n",
    "        backbone.out_channels = 2048\n",
    "\n",
    "        anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "                                           aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "\n",
    "        model = FasterRCNN(backbone, rpn_anchor_generator=anchor_generator,\n",
    "                           box_predictor=FastRCNNPredictor(1024, num_classes))\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid(rle, shape=(768,768)):\n",
    "    width, height = shape\n",
    "    xmin, ymin, xmax, ymax = rle2bbox(rle, shape)\n",
    "    if xmin >= 0 and xmax <= width and xmin < xmax and \\\n",
    "    ymin >= 0 and ymax <= height and ymin < ymax:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ship_dir = '../dev/'\n",
    "train_image_dir = os.path.join(ship_dir, 'imgs/')\n",
    "valid_image_dir = os.path.join(ship_dir, 'imgs/')\n",
    "masks = pd.read_csv(os.path.join(ship_dir,\n",
    "                                 'train_ship_segmentations_v2.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp = list(masks.groupby('ImageId'))\n",
    "image_ids =  [_id for _id, _ in grp] \n",
    "image_masks = [m['EncodedPixels'].values for _,m in grp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_target(in_mask_list, N, shape=(768, 768)):\n",
    "    if N == 0:\n",
    "        target = {}\n",
    "        target[\"boxes\"] = torch.zeros((0, 4), dtype=torch.float32)\n",
    "        target[\"labels\"] = torch.zeros((0), dtype=torch.int64)\n",
    "        return target\n",
    "    bbox_array = np.empty((N, 4), dtype=np.float32)\n",
    "    labels = torch.ones((N,), dtype=torch.int64)\n",
    "    i = 0\n",
    "    for rle in in_mask_list:\n",
    "        if isinstance(rle, str):\n",
    "            # bbox = tuple(x1, y1, x2, y2)\n",
    "            bbox = rle2bbox(rle, shape)\n",
    "            bbox_array[i,:] = bbox\n",
    "        i += 1\n",
    "    target = {\n",
    "        'boxes': torch.from_numpy(bbox_array),\n",
    "        'labels': labels,\n",
    "    }\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boxes': tensor([[469., 287., 491., 307.],\n",
       "         [ 67., 377.,  84., 386.],\n",
       "         [258., 174., 305., 185.],\n",
       "         [ 72., 386.,  78., 387.],\n",
       "         [331., 178., 369., 197.]]),\n",
       " 'labels': tensor([1, 1, 1, 1, 1])}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_mask_list = image_masks[3]\n",
    "N = sum([1 for i in in_mask_list if isinstance(i, str)])\n",
    "make_target(in_mask_list, N, shape=(768,768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ship_dir = '../dev/'\n",
    "train_image_dir = os.path.join(ship_dir, 'imgs/')\n",
    "valid_image_dir = os.path.join(ship_dir, 'imgs/')\n",
    "masks = pd.read_csv(os.path.join(ship_dir,\n",
    "                                 'train_ship_segmentations_v2.csv'))\n",
    "unique_img_ids = masks.groupby('ImageId').reset_index(name='counts')\n",
    "train_ids, valid_ids = train_test_split(unique_img_ids, \n",
    "                 test_size = 0.01, \n",
    "                 stratify = unique_img_ids['counts'],\n",
    "                 random_state=seed\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00003e153.jpg</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001124c7.jpg</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000155de5.jpg</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000194a2d.jpg</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001b1832.jpg</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ImageId  counts\n",
       "0  00003e153.jpg    True\n",
       "1  0001124c7.jpg    True\n",
       "2  000155de5.jpg    True\n",
       "3  000194a2d.jpg    True\n",
       "4  0001b1832.jpg    True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_masks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids, valid_ids = train_test_split(unique_img_ids, \n",
    "                 test_size = 0.01, \n",
    "                 stratify = unique_img_ids['counts'],\n",
    "                 random_state=seed\n",
    "                )\n",
    "print(\"Train Size: %d\" % len(train_ids))\n",
    "print(\"Valid Size: %d\" % len(valid_ids))\n",
    "train_df = pd.merge(unique_img_ids, train_ids)\n",
    "valid_df = pd.merge(unique_img_ids, valid_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from typing import Callable, Iterator, Union, Optional, List, Tuple, Dict\n",
    "\n",
    "\n",
    "def get_masks(ship_dir: str, \n",
    "                train_image_dir: Union[str, pathlib.Path], \n",
    "                valid_image_dir: Union[str, pathlib.Path]\n",
    "               ) -> pd.DataFrame:\n",
    "    masks = pd.read_csv(os.path.join(ship_dir,\n",
    "                                     'train_ship_segmentations_v2.csv'\n",
    "                                    )\n",
    "                       )\n",
    "    return masks\n",
    "\n",
    "\n",
    "def is_valid(rle, shape=(768,768)) -> bool:\n",
    "    width, height = shape\n",
    "    xmin, ymin, xmax, ymax = rle2bbox(rle, shape)\n",
    "    if xmin >= 0 and xmax <= width and xmin < xmax and \\\n",
    "    ymin >= 0 and ymax <= height and ymin < ymax:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def filter_masks(masks: pd.DataFrame) -> Tuple[dict, dict]:\n",
    "    grp = list(masks.groupby('ImageId'))\n",
    "    image_names =  {idx: filename for idx, (filename, _) in enumerate(grp)} \n",
    "    image_masks = {idx: m['EncodedPixels'].values for idx, (_, m) in enumerate(grp)}\n",
    "    to_remove = []\n",
    "    for idx, in_mask_list in image_masks.items():\n",
    "        N = sum([1 for i in in_mask_list if isinstance(i, str)])\n",
    "        if N > 0:\n",
    "            for i, rle in enumerate(in_mask_list):\n",
    "                if not is_valid(rle):\n",
    "                    to_remove.append(idx)\n",
    "                    \n",
    "    for idx in to_remove:\n",
    "        del image_names[idx]\n",
    "        del image_masks[idx]\n",
    "    return image_names, image_masks\n",
    "        \n",
    "\n",
    "def get_train_valid_dfs(masks: dict, seed: int = 0) -> Tuple[list, list, list, list]:\n",
    "    ids = np.array(list(masks.keys())).reshape((len(masks),1))\n",
    "    train_ids, valid_ids = train_test_split(\n",
    "         ids, \n",
    "         test_size = 0.01, \n",
    "         random_state=seed\n",
    "        )\n",
    "    train_ids, valid_ids = list(train_ids.flatten()), list(valid_ids.flatten())\n",
    "    train_masks = [masks[idx] for idx in train_ids]\n",
    "    valid_masks = [masks[idx] for idx in valid_ids]\n",
    "    return train_ids, train_masks, valid_ids, valid_masks\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ship_dir = '../dev/'\n",
    "    train_image_dir = os.path.join(ship_dir, 'imgs/')\n",
    "    valid_image_dir = os.path.join(ship_dir, 'imgs/')\n",
    "    masks = get_masks(ship_dir, train_image_dir, valid_image_dir)\n",
    "    image_names, filtered_masks = filter_masks(masks)\n",
    "    train_ids, train_masks, valid_ids, valid_masks = get_train_valid_dfs(\n",
    "        filtered_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "190620"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_mask_list = train_masks[0]\n",
    "N = sum([1 for i in in_mask_list if isinstance(i, str)])\n",
    "\n",
    "make_target(in_mask_list, N, shape=(768, 768))['boxes'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([420]), array([1189998119991197253])]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = np.array([[1], [2]])\n",
    "print(list(ids.flatten()))\n",
    "masks = np.array([69, 420, 1189998119991197253])\n",
    "[masks[idx] for idx in ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.functional import resize\n",
    "\n",
    "\n",
    "class Resize:\n",
    "    def __init__(self, \n",
    "                 input_shape = (768, 768), \n",
    "                 output_shape = (299, 299), \n",
    "                 interpolation=2\n",
    "                ):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        self.interpolation = interpolation\n",
    "        \n",
    "        \n",
    "    def resize_boxes(self, boxes: torch.tensor) -> torch.tensor:\n",
    "        x_orig, y_orig = self.input_shape\n",
    "        x_new, y_new = self.output_shape\n",
    "        x_scale = x_new / x_orig\n",
    "        y_scale = y_new / y_orig\n",
    "        # bbox = tuple(x1, y1, x2, y2)\n",
    "        row_scaler = torch.tensor([x_scale, y_scale, x_scale, y_scale])\n",
    "        boxes_scaled = torch.round(boxes * row_scaler).int() # Converts to new coordinates\n",
    "        return boxes_scaled\n",
    "        \n",
    "        \n",
    "    def __call__(self, image, target) -> Tuple[torch.tensor, dict]:\n",
    "        image = resize(image, size=self.output_shape, interpolation=self.interpolation)\n",
    "        target['boxes'] = self.resize_boxes(target['boxes'])\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test passed\n"
     ]
    }
   ],
   "source": [
    "def test_resize():\n",
    "    rgb_path = r'../dev/imgs/0002756f7.jpg'\n",
    "    target = {}\n",
    "    target['boxes'] = torch.tensor([[100,100, 200, 200],[10,11,12,13],[14,15,16,17]])\n",
    "    image =  Image.open(rgb_path)\n",
    "    Resize()(image, target)\n",
    "    print(\"test passed\")\n",
    "\n",
    "test_resize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test `VesselDataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomBlur:\n",
    "    def __init__(self, p=0.5, radius=2):\n",
    "        self.p = p\n",
    "        self.radius = radius\n",
    "\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        prob = np.random.rand(1)[0]\n",
    "        if prob < self.p:\n",
    "            x = x.filter(ImageFilter.GaussianBlur(self.radius))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VesselDataset(Dataset):\n",
    "    def __init__(self, boxes: Optional[list], image_names: list, train_image_dir=None, valid_image_dir=None, \n",
    "                 test_image_dir=None, transform=None, mode='train', binary=True):\n",
    "        self.boxes = boxes\n",
    "        self.image_names = image_names\n",
    "        self.train_image_dir = train_image_dir\n",
    "        self.valid_image_dir = valid_image_dir\n",
    "        self.test_image_dir = test_image_dir\n",
    "\n",
    "        mean = [0.485, 0.456, 0.406]\n",
    "        std = [0.229, 0.224, 0.225]\n",
    "        if transform is not None:\n",
    "            self.train_transform = transform\n",
    "        else:\n",
    "            self.train_transform = Compose([\n",
    "                RandomBlur(p=0.95, radius=2),\n",
    "                ToTensor(),\n",
    "                Normalize(mean, std) # Apply to all input images\n",
    "            ])\n",
    "        self.valid_transform = Compose([\n",
    "            RandomBlur(p=1.0, radius=2), # Blur all images\n",
    "            ToTensor(),\n",
    "            Normalize(mean, std) # Apply to all input images\n",
    "        ])\n",
    "        self.test_transform = Compose([\n",
    "            transforms.Resize(size=(299,299), interpolation=2),\n",
    "            ToTensor(),\n",
    "            Normalize(mean, std) # Apply to all input images\n",
    "        ])\n",
    "        self.mode = mode\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.boxes)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_file_name = self.image_names[idx]\n",
    "        if self.mode == 'train':\n",
    "            img_path = os.path.join(self.train_image_dir, img_file_name)\n",
    "        elif self.mode == 'valid':\n",
    "            img_path = os.path.join(self.valid_image_dir, img_file_name)\n",
    "        else:\n",
    "            img_path = os.path.join(self.test_image_dir, img_file_name)\n",
    "\n",
    "        #img = imread(img_path)\n",
    "        img = Image.open(img_path)\n",
    "        if self.mode =='train' or self.mode =='valid':\n",
    "            img_boxes = self.boxes[idx]\n",
    "            N = sum([1 for i in img_boxes if isinstance(i, str)])\n",
    "            target = make_target(img_boxes, N, shape=(768, 768))\n",
    "            img, target = Resize(input_shape = (768, 768), \n",
    "                                 output_shape = (299, 299)\n",
    "                                )(img, target)\n",
    "        \n",
    "        if self.mode =='train':\n",
    "            img = self.train_transform(img)\n",
    "            return img, target\n",
    "        elif self.mode == 'valid':\n",
    "            img = self.valid_transform(img)\n",
    "            return img, target\n",
    "        else:\n",
    "            img = self.test_transform(img)\n",
    "            return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for ids, name in image_names.items():  # for name, age in dictionary.iteritems():  (for Python 2.x)\n",
    "    if name == r'000d26c17.jpg':\n",
    "        print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000d26c17.jpg\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 299, 299])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VesselDataset(train_masks, image_names, test_image_dir = r'../dev/imgs', mode='test').__getitem__(32).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Everything Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from typing import Callable, Iterator, Union, Optional, List, Tuple, Dict\n",
    "from torchvision.transforms.functional import resize\n",
    "\n",
    "\n",
    "def rle2bbox(rle, shape):\n",
    "    '''\n",
    "    rle: run-length encoded image mask, as string\n",
    "    shape: (height, width) of image on which RLE was produced\n",
    "    Returns (x0, y0, x1, y1) tuple describing the bounding box of the rle mask\n",
    "    \n",
    "    Note on image vs np.array dimensions:\n",
    "    \n",
    "        np.array implies the `[y, x]` indexing order in terms of image dimensions,\n",
    "        so the variable on `shape[0]` is `y`, and the variable on the `shape[1]` is `x`,\n",
    "        hence the result would be correct (x0,y0,x1,y1) in terms of image dimensions\n",
    "        for RLE-encoded indices of np.array (which are produced by widely used kernels\n",
    "        and are used in most kaggle competitions datasets)\n",
    "    '''\n",
    "    \n",
    "    a = np.fromiter(rle.split(), dtype=np.uint)\n",
    "    a = a.reshape((-1, 2))  # an array of (start, length) pairs\n",
    "    a[:,0] -= 1  # `start` is 1-indexed\n",
    "    \n",
    "    y0 = a[:,0] % shape[0]\n",
    "    y1 = y0 + a[:,1]\n",
    "    if np.any(y1 > shape[0]):\n",
    "        # got `y` overrun, meaning that there are a pixels in mask on 0 and shape[0] position\n",
    "        y0 = 0\n",
    "        y1 = shape[0]\n",
    "    else:\n",
    "        y0 = np.min(y0)\n",
    "        y1 = np.max(y1)\n",
    "    \n",
    "    x0 = a[:,0] // shape[0]\n",
    "    x1 = (a[:,0] + a[:,1]) // shape[0]\n",
    "    x0 = np.min(x0)\n",
    "    x1 = np.max(x1)\n",
    "    \n",
    "    if x1 > shape[1]:\n",
    "        # just went out of the image dimensions\n",
    "        raise ValueError(\"invalid RLE or image dimensions: x1=%d > shape[1]=%d\" % (\n",
    "            x1, shape[1]\n",
    "        ))\n",
    "\n",
    "    return x0, y0, x1, y1\n",
    "\n",
    "\n",
    "# From: https://www.kaggle.com/paulorzp/run-length-encode-and-decode\n",
    "def rle_decode(mask_rle, shape=(299, 768)):\n",
    "    '''\n",
    "    mask_rle: run-length as string formated (start length)\n",
    "    shape: (height,width) of array to return \n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "    '''\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape).T  # Needed to align to RLE direction\n",
    "\n",
    "\n",
    "def is_valid(rle, shape=(768,768)):\n",
    "    width, height = shape\n",
    "    xmin, ymin, xmax, ymax = rle2bbox(rle, shape)\n",
    "    if xmin >= 0 and xmax <= width and xmin < xmax and \\\n",
    "    ymin >= 0 and ymax <= height and ymin < ymax:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def make_target(in_mask_list, N, shape=(768, 768)):\n",
    "    if N == 0:\n",
    "        target = {}\n",
    "        target[\"boxes\"] = torch.zeros((0, 4), dtype=torch.float32)\n",
    "        target[\"labels\"] = torch.zeros((0), dtype=torch.int64)\n",
    "        return target\n",
    "    bbox_array = np.empty((N, 4), dtype=np.float32)\n",
    "    labels = torch.ones((N,), dtype=torch.int64)\n",
    "    i = 0\n",
    "    for rle in in_mask_list:\n",
    "        if isinstance(rle, str):\n",
    "            # bbox = tuple(x1, y1, x2, y2)\n",
    "            bbox = rle2bbox(rle, shape)\n",
    "            bbox_array[i,:] = bbox\n",
    "        i += 1\n",
    "    target = {\n",
    "        'boxes': torch.from_numpy(bbox_array),\n",
    "        'labels': labels,\n",
    "    }\n",
    "    return target\n",
    "\n",
    "\n",
    "def get_masks(ship_dir: str, \n",
    "                train_image_dir: Union[str, pathlib.Path], \n",
    "                valid_image_dir: Union[str, pathlib.Path]\n",
    "               ) -> pd.DataFrame:\n",
    "    masks = pd.read_csv(os.path.join(ship_dir,\n",
    "                                     'train_ship_segmentations_v2.csv'\n",
    "                                    )\n",
    "                       )\n",
    "    return masks\n",
    "\n",
    "\n",
    "def is_valid(rle, shape=(768,768)) -> bool:\n",
    "    width, height = shape\n",
    "    xmin, ymin, xmax, ymax = rle2bbox(rle, shape)\n",
    "    if xmin >= 0 and xmax <= width and xmin < xmax and \\\n",
    "    ymin >= 0 and ymax <= height and ymin < ymax:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def filter_masks(masks: pd.DataFrame) -> Tuple[dict, dict]:\n",
    "    grp = list(masks.groupby('ImageId'))\n",
    "    image_names =  {idx: filename for idx, (filename, _) in enumerate(grp)} \n",
    "    image_masks = {idx: m['EncodedPixels'].values for idx, (_, m) in enumerate(grp)}\n",
    "    to_remove = []\n",
    "    for idx, in_mask_list in image_masks.items():\n",
    "        N = sum([1 for i in in_mask_list if isinstance(i, str)])\n",
    "        if N > 0:\n",
    "            for i, rle in enumerate(in_mask_list):\n",
    "                if not is_valid(rle):\n",
    "                    to_remove.append(idx)\n",
    "                    \n",
    "    for idx in to_remove:\n",
    "        del image_names[idx]\n",
    "        del image_masks[idx]\n",
    "    return image_names, image_masks\n",
    "        \n",
    "\n",
    "def get_train_valid_dfs(masks: dict, seed: int = 0) -> Tuple[list, list, list, list]:\n",
    "    ids = np.array(list(masks.keys())).reshape((len(masks),1))\n",
    "    train_ids, valid_ids = train_test_split(\n",
    "         ids, \n",
    "         test_size = 0.01, \n",
    "         random_state=seed\n",
    "        )\n",
    "    train_ids, valid_ids = list(train_ids.flatten()), list(valid_ids.flatten())\n",
    "    train_masks = [masks[idx] for idx in train_ids]\n",
    "    valid_masks = [masks[idx] for idx in valid_ids]\n",
    "    return train_ids, train_masks, valid_ids, valid_masks\n",
    "\n",
    "\n",
    "class Resize:\n",
    "    def __init__(self, \n",
    "                 input_shape = (768, 768), \n",
    "                 output_shape = (299, 299), \n",
    "                 interpolation=2\n",
    "                ):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        self.interpolation = interpolation\n",
    "        \n",
    "        \n",
    "    def resize_boxes(self, boxes: torch.tensor) -> torch.tensor:\n",
    "        x_orig, y_orig = self.input_shape\n",
    "        x_new, y_new = self.output_shape\n",
    "        x_scale = x_new / x_orig\n",
    "        y_scale = y_new / y_orig\n",
    "        # bbox = tuple(x1, y1, x2, y2)\n",
    "        row_scaler = torch.tensor([x_scale, y_scale, x_scale, y_scale])\n",
    "        boxes_scaled = torch.round(boxes * row_scaler).int() # Converts to new coordinates\n",
    "        return boxes_scaled\n",
    "        \n",
    "        \n",
    "    def __call__(self, image, target) -> Tuple[torch.tensor, dict]:\n",
    "        image = resize(image, size=self.output_shape, interpolation=self.interpolation)\n",
    "        target['boxes'] = self.resize_boxes(target['boxes'])\n",
    "        return image, target\n",
    "    \n",
    "    \n",
    "class RandomBlur:\n",
    "    def __init__(self, p=0.5, radius=2):\n",
    "        self.p = p\n",
    "        self.radius = radius\n",
    "\n",
    "\n",
    "    def __call__(self, x):\n",
    "        prob = np.random.rand(1)[0]\n",
    "        if prob < self.p:\n",
    "            x = x.filter(ImageFilter.GaussianBlur(self.radius))\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class VesselDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 boxes: dict, \n",
    "                 image_ids: list,\n",
    "                 image_names: dict, \n",
    "                 train_image_dir=None, \n",
    "                 valid_image_dir=None, \n",
    "                 test_image_dir=None, \n",
    "                 transform=None, \n",
    "                 mode='train', \n",
    "                 binary=True):\n",
    "        self.boxes = boxes\n",
    "        self.image_ids = image_ids\n",
    "        self.image_names = image_names\n",
    "        self.train_image_dir = train_image_dir\n",
    "        self.valid_image_dir = valid_image_dir\n",
    "        self.test_image_dir = test_image_dir\n",
    "\n",
    "        mean = [0.485, 0.456, 0.406]\n",
    "        std = [0.229, 0.224, 0.225]\n",
    "        if transform is not None:\n",
    "            self.train_transform = transform\n",
    "        else:\n",
    "            self.train_transform = Compose([\n",
    "                RandomBlur(p=0.95, radius=2),\n",
    "                ToTensor(),\n",
    "                Normalize(mean, std) # Apply to all input images\n",
    "            ])\n",
    "        self.valid_transform = Compose([\n",
    "            RandomBlur(p=1.0, radius=2), # Blur all images\n",
    "            ToTensor(),\n",
    "            Normalize(mean, std) # Apply to all input images\n",
    "        ])\n",
    "        self.test_transform = Compose([\n",
    "            transforms.Resize(size=(299,299), interpolation=2),\n",
    "            ToTensor(),\n",
    "            Normalize(mean, std) # Apply to all input images\n",
    "        ])\n",
    "        self.mode = mode\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx = self.image_ids[idx] # Convert from input to image ID number\n",
    "        img_file_name = self.image_names[idx]\n",
    "        if self.mode == 'train':\n",
    "            img_path = os.path.join(self.train_image_dir, img_file_name)\n",
    "        elif self.mode == 'valid':\n",
    "            img_path = os.path.join(self.valid_image_dir, img_file_name)\n",
    "        else:\n",
    "            img_path = os.path.join(self.test_image_dir, img_file_name)\n",
    "\n",
    "        #img = imread(img_path)\n",
    "        img = Image.open(img_path)\n",
    "        if self.mode =='train' or self.mode =='valid':\n",
    "            img_boxes = self.boxes[idx]\n",
    "            N = sum([1 for i in img_boxes if isinstance(i, str)])\n",
    "            target = make_target(img_boxes, N, shape=(768, 768))\n",
    "            img, target = Resize(input_shape = (768, 768), \n",
    "                                 output_shape = (299, 299)\n",
    "                                )(img, target)\n",
    "            # Make image_id\n",
    "            image_id = torch.tensor([idx])\n",
    "            target[\"image_id\"] = image_id\n",
    "        \n",
    "        if self.mode =='train':\n",
    "            img = self.train_transform(img)\n",
    "            return img, target\n",
    "        elif self.mode == 'valid':\n",
    "            img = self.valid_transform(img)\n",
    "            return img, target\n",
    "        else:\n",
    "            img = self.test_transform(img)\n",
    "            return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transforms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-fdacc47eed2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mvessel_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVesselDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_image_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr'../dev/imgs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mvessel_valid_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVesselDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_image_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr'../dev/imgs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-9de2fea5b0cc>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, boxes, image_names, train_image_dir, valid_image_dir, test_image_dir, transform, mode, binary)\u001b[0m\n\u001b[1;32m    219\u001b[0m         ])\n\u001b[1;32m    220\u001b[0m         self.test_transform = Compose([\n\u001b[0;32m--> 221\u001b[0;31m             \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m299\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m299\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m             \u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mNormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Apply to all input images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'transforms' is not defined"
     ]
    }
   ],
   "source": [
    "ship_dir = '../dev/'\n",
    "train_image_dir = os.path.join(ship_dir, 'imgs/')\n",
    "valid_image_dir = os.path.join(ship_dir, 'imgs/')\n",
    "masks = get_masks(ship_dir, train_image_dir, valid_image_dir)\n",
    "image_names, filtered_masks = filter_masks(masks)\n",
    "train_ids, train_masks, valid_ids, valid_masks = get_train_valid_dfs(\n",
    "    filtered_masks\n",
    ")\n",
    "\n",
    "vessel_dataset = VesselDataset(train_masks, image_names, train_image_dir = r'../dev/imgs', mode='train')\n",
    "\n",
    "vessel_valid_dataset = VesselDataset(valid_masks, image_names, valid_image_dir = r'../dev/imgs', mode='valid')\n",
    "\n",
    "batch_size = 64\n",
    "shuffle = True\n",
    "loader = DataLoader(\n",
    "            dataset=vessel_dataset,\n",
    "            shuffle=shuffle,\n",
    "            #num_workers = 0,\n",
    "            batch_size=batch_size,\n",
    "            pin_memory=torch.cuda.is_available()\n",
    "        )\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "            dataset=vessel_valid_dataset,\n",
    "            shuffle=shuffle,\n",
    "            #num_workers = 0,\n",
    "            batch_size=batch_size,\n",
    "            pin_memory=torch.cuda.is_available()\n",
    "        )\n",
    "\n",
    "num_epochs = 30\n",
    "print_freq = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dummy Model and Test IO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://discuss.pytorch.org/t/faster-rcnn-with-inceptionv3-backbone-very-slow/91455\n",
    "def make_model(state_dict=None, num_classes=2):\n",
    "        inception = torchvision.models.inception_v3(pretrained=False, progress=False, \n",
    "                                                    num_classes=num_classes, aux_logits=False)\n",
    "        #inception.load_state_dict(torch.load(state_dict))\n",
    "        modules = list(inception.children())[:-1]\n",
    "        backbone = nn.Sequential(*modules)\n",
    "\n",
    "        for layer in backbone:\n",
    "            for p in layer.parameters():\n",
    "                p.requires_grad = False # Freezes the backbone layers\n",
    "\n",
    "        backbone.out_channels = 2048\n",
    "\n",
    "        anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "                                           aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "\n",
    "        model = FasterRCNN(backbone, rpn_anchor_generator=anchor_generator,\n",
    "                           box_predictor=FastRCNNPredictor(1024, num_classes))\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/poisson/lib/python3.7/site-packages/torchvision/models/inception.py:77: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
      "  ' due to scipy/scipy#11299), please set init_weights=True.', FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "model = make_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reformat `train_one_epoch` and `evaluate` For New Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp vision/references/detection/utils.py ./\n",
    "!cp vision/references/detection/transforms.py ./\n",
    "!cp vision/references/detection/coco_eval.py ./\n",
    "!cp vision/references/detection/engine.py ./\n",
    "!cp vision/references/detection/coco_utils.py ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pycocotools\n",
      "  Downloading pycocotools-2.0.2.tar.gz (23 kB)\n",
      "Requirement already satisfied: setuptools>=18.0 in /opt/miniconda3/envs/poisson/lib/python3.7/site-packages (from pycocotools) (47.3.0.post20200616)\n",
      "Collecting cython>=0.27.3\n",
      "  Using cached Cython-0.29.21-cp37-cp37m-macosx_10_9_x86_64.whl (1.9 MB)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in /opt/miniconda3/envs/poisson/lib/python3.7/site-packages (from pycocotools) (3.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/miniconda3/envs/poisson/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (0.10.0)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /opt/miniconda3/envs/poisson/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (2020.6.20)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/miniconda3/envs/poisson/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.15 in /opt/miniconda3/envs/poisson/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (1.18.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /opt/miniconda3/envs/poisson/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (2.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/miniconda3/envs/poisson/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (8.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/miniconda3/envs/poisson/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (2.8.1)\n",
      "Requirement already satisfied: six in /opt/miniconda3/envs/poisson/lib/python3.7/site-packages (from cycler>=0.10->matplotlib>=2.1.0->pycocotools) (1.15.0)\n",
      "Building wheels for collected packages: pycocotools\n",
      "  Building wheel for pycocotools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pycocotools: filename=pycocotools-2.0.2-cp37-cp37m-macosx_10_9_x86_64.whl size=90488 sha256=3d5863d3d1301f7eea1066a95f84b1df9912c97b06356d1e05afd3e7576ef152\n",
      "  Stored in directory: /Users/richardcorrero/Library/Caches/pip/wheels/bc/cf/1b/e95c99c5f9d1648be3f500ca55e7ce55f24818b0f48336adaf\n",
      "Successfully built pycocotools\n",
      "Installing collected packages: cython, pycocotools\n",
      "Successfully installed cython-0.29.21 pycocotools-2.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import torchvision.transforms as transforms\n",
    "from engine import train_one_epoch, evaluate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/poisson/lib/python3.7/site-packages/torchvision/models/inception.py:77: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
      "  ' due to scipy/scipy#11299), please set init_weights=True.', FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'error' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-e8f25b3cc3ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         train_one_epoch(model, optimizer, data_loader, device, epoch, lr_scheduler = None, \n\u001b[0m\u001b[1;32m     48\u001b[0m                         print_every = 100, num_epochs = 30)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_loader' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-e8f25b3cc3ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m         train_one_epoch(model, optimizer, data_loader, device, epoch, lr_scheduler = None, \n\u001b[1;32m     48\u001b[0m                         print_every = 100, num_epochs = 30)\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0;32mexcept\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'error' is not defined"
     ]
    }
   ],
   "source": [
    "ship_dir = '../dev/'\n",
    "train_image_dir = os.path.join(ship_dir, 'imgs/')\n",
    "valid_image_dir = os.path.join(ship_dir, 'imgs/')\n",
    "masks = get_masks(ship_dir, train_image_dir, valid_image_dir)\n",
    "image_names, filtered_masks = filter_masks(masks)\n",
    "train_ids, train_masks, valid_ids, valid_masks = get_train_valid_dfs(\n",
    "    filtered_masks\n",
    ")\n",
    "\n",
    "vessel_dataset = VesselDataset(train_masks, image_names, train_image_dir = r'../dev/imgs', mode='train')\n",
    "\n",
    "vessel_valid_dataset = VesselDataset(valid_masks, image_names, valid_image_dir = r'../dev/imgs', mode='valid')\n",
    "\n",
    "batch_size = 64\n",
    "shuffle = True\n",
    "loader = DataLoader(\n",
    "            dataset=vessel_dataset,\n",
    "            shuffle=shuffle,\n",
    "            #num_workers = 0,\n",
    "            batch_size=batch_size,\n",
    "            pin_memory=torch.cuda.is_available()\n",
    "        )\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "            dataset=vessel_valid_dataset,\n",
    "            shuffle=shuffle,\n",
    "            #num_workers = 0,\n",
    "            batch_size=batch_size,\n",
    "            pin_memory=torch.cuda.is_available()\n",
    "        )\n",
    "\n",
    "model = make_model()\n",
    "\n",
    "num_epochs = 30\n",
    "print_freq = 100\n",
    "\n",
    "device = torch.device('cpu')\n",
    "model = model.to(device)\n",
    "savepath = r'./sdad'\n",
    "lr = 1e-4\n",
    "weight_decay = 1e-7 # Default should be 1e-5\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "print('Starting Training...\\n')\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "    try:\n",
    "        train_one_epoch(model, optimizer, data_loader, device, epoch, lr_scheduler = None, \n",
    "                        print_every = 100, num_epochs = 30)\n",
    "    except:\n",
    "        continue\n",
    "    print('Epoch %d completed. Running validation...\\n' % (epoch + 1))\n",
    "    try:\n",
    "        metrics = evaluate(model, data_loader, device)\n",
    "    except error:\n",
    "        continue\n",
    "    print('Saving Model...\\n')\n",
    "    torch.save(model.state_dict(), savepath)\n",
    "    print('Model Saved.\\n')\n",
    "print('Finished Training.\\n')\n",
    "print('Saving Model...\\n')\n",
    "torch.save(model.state_dict(), savepath)\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rewrite Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_print(running_loss, \n",
    "                print_every, \n",
    "                batch_size, \n",
    "                epoch, \n",
    "                num_minibatches_per_epoch, \n",
    "                time_left):\n",
    "    print('[%d, %5d] Running Loss: %.3f' %\n",
    "          (epoch + 1, i + 1, (running_loss / print_every)))\n",
    "    print('           Number of Samples Seen: %d' %\n",
    "          (batch_size * ((i + 1) + epoch * num_minibatches_per_epoch)))\n",
    "    print('           Estimated Hours Remaining: %.2f\\n' % time_left)\n",
    "\n",
    "\n",
    "def train_one_epoch(model, \n",
    "                    optimizer, \n",
    "                    data_loader, \n",
    "                    device, \n",
    "                    epoch, \n",
    "                    lr_scheduler = None, \n",
    "                    print_every = 100,\n",
    "                    num_epochs = 30):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    minibatch_time = 0.0\n",
    "#    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
    "#    header = 'Epoch: [{}]'.format(epoch)\n",
    "#\n",
    "#    lr_scheduler = None\n",
    "#    if epoch == 0:\n",
    "#        warmup_factor = 1. / 1000\n",
    "#        warmup_iters = min(1000, len(data_loader) - 1)\n",
    "#\n",
    "#        lr_scheduler = utils.warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor)\n",
    "\n",
    "    for i, (inputs, targets) in enumerate(data_loader):\n",
    "        start = time.time()\n",
    "#        images = list(image.to(device) for image in images)\n",
    "        inputs = Variable(inputs).cuda()\n",
    "        targets = [{k: Variable(v).cuda() for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(inputs, targets)\n",
    "\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        #        # reduce losses over all GPUs for logging purposes\n",
    "        #        loss_dict_reduced = utils.reduce_dict(loss_dict)\n",
    "        #        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
    "\n",
    "        #        loss_value = losses_reduced.item()\n",
    "\n",
    "        if not math.isfinite(losses):\n",
    "            print(\"Loss is %-10.5f, stopping training\".format(losses))\n",
    "            sys.exit(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        end = time.time()\n",
    "        minibatch_time += float(end - start)\n",
    "        if (i + 1) % print_every == 0:\n",
    "            minibatch_time = minibatch_time / (3600.0 * print_every)\n",
    "            num_minibatches_left = 1.01 * len(data_loader) - (i + 1)\n",
    "            num_minibatches_per_epoch = 1.01 * len(data_loader) - 1 + \\\n",
    "            ((len(dataloader.dataset) % batch_size) / batch_size)\n",
    "            num_epochs_left = num_epochs - (epoch + 1)\n",
    "            time_left = minibatch_time * \\\n",
    "                (num_minibatches_left + num_epochs_left * num_minibatches_per_epoch)\n",
    "            time_left *= 6.0 # Adjust for timing discrepencies\n",
    "            train_print(running_loss, print_every, batch_size, epoch, \n",
    "                        num_minibatches_per_epoch, time_left)\n",
    "            running_loss = 0.0\n",
    "            minibatch_time = 0.0\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data_loader):\n",
    "    n_threads = torch.get_num_threads()\n",
    "    torch.set_num_threads(n_threads)\n",
    "    cpu_device = torch.device(\"cpu\")\n",
    "    model.eval()\n",
    "\n",
    "    coco = get_coco_api_from_dataset(data_loader.dataset)\n",
    "    iou_types = _get_iou_types(model)\n",
    "    coco_evaluator = CocoEvaluator(coco, iou_types)\n",
    "\n",
    "    for images, targets in metric_logger.log_every(data_loader, 100, header):\n",
    "        images = list(Variable(img).to(device) for img in images)\n",
    "\n",
    "        model_time = time.time()\n",
    "        outputs = model(images)\n",
    "\n",
    "        outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
    "        model_time = time.time() - model_time\n",
    "\n",
    "        res = {target[\"image_id\"].item(): output for target, output in zip(targets, outputs)}\n",
    "        evaluator_time = time.time()\n",
    "        coco_evaluator.update(res)\n",
    "        evaluator_time = time.time() - evaluator_time\n",
    "        metric_logger.update(model_time=model_time, evaluator_time=evaluator_time)\n",
    "\n",
    "    # gather the stats from all processes\n",
    "    metric_logger.synchronize_between_processes()\n",
    "    print(\"Averaged stats:\", metric_logger)\n",
    "    coco_evaluator.synchronize_between_processes()\n",
    "\n",
    "    # accumulate predictions from all images\n",
    "    coco_evaluator.accumulate()\n",
    "    coco_evaluator.summarize()\n",
    "    torch.set_num_threads(n_threads)\n",
    "    return coco_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Starting Training...\\n')\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, lr_scheduler = None, \n",
    "                    print_every = 100, num_epochs = 30)\n",
    "    print('Epoch %d completed. Running validation...\\n' % (epoch + 1))\n",
    "    metrics = evaluate(model, data_loader, device)\n",
    "    print('Saving Model...\\n')\n",
    "    torch.save(model.state_dict(), savepath)\n",
    "    print('Model Saved.\\n')\n",
    "print('Finished Training.\\n')\n",
    "print('Saving Model...\\n')\n",
    "torch.save(model.state_dict(), savepath)\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/poisson/lib/python3.7/site-packages/torchvision/models/inception.py:77: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
      "  ' due to scipy/scipy#11299), please set init_weights=True.', FutureWarning)\n",
      "Train Size: 190620\n",
      "Valid Size: 1926\n",
      "Starting Training...\n",
      "\n",
      "Epoch 1 completed. Running validation...\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"../vessel_detector_test_suite.py\", line 494, in <module>\n",
      "    main()\n",
      "  File \"../vessel_detector_test_suite.py\", line 481, in main\n",
      "    metrics = evaluate(model, valid_loader, device)\n",
      "  File \"/opt/miniconda3/envs/poisson/lib/python3.7/site-packages/torch/autograd/grad_mode.py\", line 26, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"../vessel_detector_test_suite.py\", line 385, in evaluate\n",
      "    coco = get_coco_api_from_dataset(data_loader.dataset)\n",
      "  File \"/Users/richardcorrero/Projects/research/poisson/coco_utils.py\", line 206, in get_coco_api_from_dataset\n",
      "    return convert_to_coco_api(dataset)\n",
      "  File \"/Users/richardcorrero/Projects/research/poisson/coco_utils.py\", line 155, in convert_to_coco_api\n",
      "    img, targets = ds[img_idx]\n",
      "  File \"../vessel_detector_test_suite.py\", line 270, in __getitem__\n",
      "    img = Image.open(img_path)\n",
      "  File \"/opt/miniconda3/envs/poisson/lib/python3.7/site-packages/PIL/Image.py\", line 2891, in open\n",
      "    fp = builtins.open(filename, \"rb\")\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '../dev/imgs/64d82cd21.jpg'\n"
     ]
    }
   ],
   "source": [
    "! python ../vessel_detector_test_suite.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
