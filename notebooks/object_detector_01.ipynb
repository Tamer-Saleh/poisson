{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
    "from torchvision.models.detection.faster_rcnn import FasterRCNN, FastRCNNPredictor\n",
    "from torchvision.transforms import ToTensor, Compose, RandomHorizontalFlip,\\\n",
    "    RandomVerticalFlip, Resize, Normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image, ImageFile, ImageFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle2bbox(rle, shape):\n",
    "    '''\n",
    "    rle: run-length encoded image mask, as string\n",
    "    shape: (height, width) of image on which RLE was produced\n",
    "    Returns (x0, y0, x1, y1) tuple describing the bounding box of the rle mask\n",
    "    \n",
    "    Note on image vs np.array dimensions:\n",
    "    \n",
    "        np.array implies the `[y, x]` indexing order in terms of image dimensions,\n",
    "        so the variable on `shape[0]` is `y`, and the variable on the `shape[1]` is `x`,\n",
    "        hence the result would be correct (x0,y0,x1,y1) in terms of image dimensions\n",
    "        for RLE-encoded indices of np.array (which are produced by widely used kernels\n",
    "        and are used in most kaggle competitions datasets)\n",
    "    '''\n",
    "    \n",
    "    a = np.fromiter(rle.split(), dtype=np.uint)\n",
    "    a = a.reshape((-1, 2))  # an array of (start, length) pairs\n",
    "    a[:,0] -= 1  # `start` is 1-indexed\n",
    "    \n",
    "    y0 = a[:,0] % shape[0]\n",
    "    y1 = y0 + a[:,1]\n",
    "    if np.any(y1 > shape[0]):\n",
    "        # got `y` overrun, meaning that there are a pixels in mask on 0 and shape[0] position\n",
    "        y0 = 0\n",
    "        y1 = shape[0]\n",
    "    else:\n",
    "        y0 = np.min(y0)\n",
    "        y1 = np.max(y1)\n",
    "    \n",
    "    x0 = a[:,0] // shape[0]\n",
    "    x1 = (a[:,0] + a[:,1]) // shape[0]\n",
    "    x0 = np.min(x0)\n",
    "    x1 = np.max(x1)\n",
    "    \n",
    "    if x1 > shape[1]:\n",
    "        # just went out of the image dimensions\n",
    "        raise ValueError(\"invalid RLE or image dimensions: x1=%d > shape[1]=%d\" % (\n",
    "            x1, shape[1]\n",
    "        ))\n",
    "\n",
    "    return x0, y0, x1, y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(state_dict, num_classes):\n",
    "        inception = torchvision.models.inception_v3(pretrained=False, progress=False, \n",
    "                                                    num_classes=num_classes, aux_logits=False)\n",
    "        inception.load_state_dict(torch.load(state_dict))\n",
    "        modules = list(inception.children())[:-1]\n",
    "        backbone = nn.Sequential(*modules)\n",
    "\n",
    "        for layer in backbone:\n",
    "            for p in layer.parameters():\n",
    "                p.requires_grad = False # Freezes the backbone layers\n",
    "\n",
    "        backbone.out_channels = 2048\n",
    "\n",
    "        anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "                                           aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "\n",
    "        model = FasterRCNN(backbone, rpn_anchor_generator=anchor_generator,\n",
    "                           box_predictor=FastRCNNPredictor(1024, num_classes))\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid(rle, shape=(768,768)):\n",
    "    width, height = shape\n",
    "    xmin, ymin, xmax, ymax = rle2bbox(rle, shape)\n",
    "    if xmin >= 0 and xmax <= width and xmin < xmax and \\\n",
    "    ymin >= 0 and ymax <= height and ymin < ymax:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "ship_dir = '../dev/'\n",
    "train_image_dir = os.path.join(ship_dir, 'imgs/')\n",
    "valid_image_dir = os.path.join(ship_dir, 'imgs/')\n",
    "masks = pd.read_csv(os.path.join(ship_dir,\n",
    "                                 'train_ship_segmentations_v2.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "\n",
    "ship_dir = '../dev/'\n",
    "train_image_dir = os.path.join(ship_dir, 'imgs/')\n",
    "mypath = train_image_dir\n",
    "img_names = [f for f in listdir(mypath) if isfile(join(mypath, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>EncodedPixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00003e153.jpg</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001124c7.jpg</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000155de5.jpg</td>\n",
       "      <td>264661 17 265429 33 266197 33 266965 33 267733...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000194a2d.jpg</td>\n",
       "      <td>360486 1 361252 4 362019 5 362785 8 363552 10 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000194a2d.jpg</td>\n",
       "      <td>51834 9 52602 9 53370 9 54138 9 54906 9 55674 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231718</th>\n",
       "      <td>fffedbb6b.jpg</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231719</th>\n",
       "      <td>ffff2aa57.jpg</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231720</th>\n",
       "      <td>ffff6e525.jpg</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231721</th>\n",
       "      <td>ffffc50b4.jpg</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231722</th>\n",
       "      <td>ffffe97f3.jpg</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>231723 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ImageId                                      EncodedPixels\n",
       "0       00003e153.jpg                                                NaN\n",
       "1       0001124c7.jpg                                                NaN\n",
       "2       000155de5.jpg  264661 17 265429 33 266197 33 266965 33 267733...\n",
       "3       000194a2d.jpg  360486 1 361252 4 362019 5 362785 8 363552 10 ...\n",
       "4       000194a2d.jpg  51834 9 52602 9 53370 9 54138 9 54906 9 55674 ...\n",
       "...               ...                                                ...\n",
       "231718  fffedbb6b.jpg                                                NaN\n",
       "231719  ffff2aa57.jpg                                                NaN\n",
       "231720  ffff6e525.jpg                                                NaN\n",
       "231721  ffffc50b4.jpg                                                NaN\n",
       "231722  ffffe97f3.jpg                                                NaN\n",
       "\n",
       "[231723 rows x 2 columns]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['000d42241.jpg',\n",
       " '000592296.jpg',\n",
       " '.DS_Store',\n",
       " '0005d01c8.jpg',\n",
       " '0002756f7.jpg',\n",
       " '0006c52e8.jpg',\n",
       " '000d26c17.jpg',\n",
       " '0017c19d6.jpg',\n",
       " '0014b1235.jpg',\n",
       " '000f7e728.jpg']"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>EncodedPixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0002756f7.jpg</td>\n",
       "      <td>255784 2 256552 4 257319 7 258087 9 258854 12 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0002756f7.jpg</td>\n",
       "      <td>248878 1 249645 4 250413 6 251180 9 251948 10 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>000592296.jpg</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0005d01c8.jpg</td>\n",
       "      <td>56010 1 56777 3 57544 6 58312 7 59079 9 59846 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0005d01c8.jpg</td>\n",
       "      <td>365871 1 366638 3 367405 6 368173 7 368940 9 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0006c52e8.jpg</td>\n",
       "      <td>146366 1 147132 4 147899 5 148666 7 149432 10 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>000d26c17.jpg</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>000d42241.jpg</td>\n",
       "      <td>369226 3 369992 5 370760 5 371528 5 372296 5 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>000f7e728.jpg</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0014b1235.jpg</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0017c19d6.jpg</td>\n",
       "      <td>329228 1 329995 3 330762 4 331529 6 332296 8 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0017c19d6.jpg</td>\n",
       "      <td>405963 1 406730 3 407497 5 408264 7 409031 10 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ImageId                                      EncodedPixels\n",
       "18  0002756f7.jpg  255784 2 256552 4 257319 7 258087 9 258854 12 ...\n",
       "19  0002756f7.jpg  248878 1 249645 4 250413 6 251180 9 251948 10 ...\n",
       "28  000592296.jpg                                                NaN\n",
       "29  0005d01c8.jpg  56010 1 56777 3 57544 6 58312 7 59079 9 59846 ...\n",
       "30  0005d01c8.jpg  365871 1 366638 3 367405 6 368173 7 368940 9 3...\n",
       "32  0006c52e8.jpg  146366 1 147132 4 147899 5 148666 7 149432 10 ...\n",
       "47  000d26c17.jpg                                                NaN\n",
       "48  000d42241.jpg  369226 3 369992 5 370760 5 371528 5 372296 5 3...\n",
       "54  000f7e728.jpg                                                NaN\n",
       "78  0014b1235.jpg                                                NaN\n",
       "89  0017c19d6.jpg  329228 1 329995 3 330762 4 331529 6 332296 8 3...\n",
       "90  0017c19d6.jpg  405963 1 406730 3 407497 5 408264 7 409031 10 ..."
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks_filtered = masks.loc[masks['ImageId'].isin(img_names)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath = ship_dir = '../dev/imgs/train_ship_segmentations_v2.csv'\n",
    "masks_filtered.to_csv(savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp = list(masks.groupby('ImageId'))\n",
    "image_ids =  [_id for _id, _ in grp] \n",
    "image_masks = [m['EncodedPixels'].values for _,m in grp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_target(in_mask_list, N, shape=(768, 768)):\n",
    "    if N == 0:\n",
    "        target = {}\n",
    "        target[\"boxes\"] = torch.zeros((0, 4), dtype=torch.float32)\n",
    "        target[\"labels\"] = torch.zeros((0), dtype=torch.int64)\n",
    "        return target\n",
    "    bbox_array = np.empty((N, 4), dtype=np.float32)\n",
    "    labels = torch.ones((N,), dtype=torch.int64)\n",
    "    i = 0\n",
    "    for rle in in_mask_list:\n",
    "        if isinstance(rle, str):\n",
    "            # bbox = tuple(x1, y1, x2, y2)\n",
    "            bbox = rle2bbox(rle, shape)\n",
    "            bbox_array[i,:] = bbox\n",
    "        i += 1\n",
    "    target = {\n",
    "        'boxes': torch.from_numpy(bbox_array),\n",
    "        'labels': labels,\n",
    "    }\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boxes': tensor([[469., 287., 491., 307.],\n",
       "         [ 67., 377.,  84., 386.],\n",
       "         [258., 174., 305., 185.],\n",
       "         [ 72., 386.,  78., 387.],\n",
       "         [331., 178., 369., 197.]]),\n",
       " 'labels': tensor([1, 1, 1, 1, 1])}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_mask_list = image_masks[3]\n",
    "N = sum([1 for i in in_mask_list if isinstance(i, str)])\n",
    "make_target(in_mask_list, N, shape=(768,768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ship_dir = '../dev/'\n",
    "train_image_dir = os.path.join(ship_dir, 'imgs/')\n",
    "valid_image_dir = os.path.join(ship_dir, 'imgs/')\n",
    "masks = pd.read_csv(os.path.join(ship_dir,\n",
    "                                 'train_ship_segmentations_v2.csv'))\n",
    "unique_img_ids = masks.groupby('ImageId').reset_index(name='counts')\n",
    "train_ids, valid_ids = train_test_split(unique_img_ids, \n",
    "                 test_size = 0.01, \n",
    "                 stratify = unique_img_ids['counts'],\n",
    "                 random_state=seed\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00003e153.jpg</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001124c7.jpg</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000155de5.jpg</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000194a2d.jpg</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001b1832.jpg</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ImageId  counts\n",
       "0  00003e153.jpg    True\n",
       "1  0001124c7.jpg    True\n",
       "2  000155de5.jpg    True\n",
       "3  000194a2d.jpg    True\n",
       "4  0001b1832.jpg    True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_masks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids, valid_ids = train_test_split(unique_img_ids, \n",
    "                 test_size = 0.01, \n",
    "                 stratify = unique_img_ids['counts'],\n",
    "                 random_state=seed\n",
    "                )\n",
    "print(\"Train Size: %d\" % len(train_ids))\n",
    "print(\"Valid Size: %d\" % len(valid_ids))\n",
    "train_df = pd.merge(unique_img_ids, train_ids)\n",
    "valid_df = pd.merge(unique_img_ids, valid_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from typing import Callable, Iterator, Union, Optional, List, Tuple, Dict\n",
    "\n",
    "\n",
    "def get_masks(ship_dir: str, \n",
    "                train_image_dir: Union[str, pathlib.Path], \n",
    "                valid_image_dir: Union[str, pathlib.Path]\n",
    "               ) -> pd.DataFrame:\n",
    "    masks = pd.read_csv(os.path.join(ship_dir,\n",
    "                                     'train_ship_segmentations_v2.csv'\n",
    "                                    )\n",
    "                       )\n",
    "    return masks\n",
    "\n",
    "\n",
    "def is_valid(rle, shape=(768,768)) -> bool:\n",
    "    width, height = shape\n",
    "    xmin, ymin, xmax, ymax = rle2bbox(rle, shape)\n",
    "    if xmin >= 0 and xmax <= width and xmin < xmax and \\\n",
    "    ymin >= 0 and ymax <= height and ymin < ymax:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def filter_masks(masks: pd.DataFrame) -> Tuple[dict, dict]:\n",
    "    grp = list(masks.groupby('ImageId'))\n",
    "    image_names =  {idx: filename for idx, (filename, _) in enumerate(grp)} \n",
    "    image_masks = {idx: m['EncodedPixels'].values for idx, (_, m) in enumerate(grp)}\n",
    "    to_remove = []\n",
    "    for idx, in_mask_list in image_masks.items():\n",
    "        N = sum([1 for i in in_mask_list if isinstance(i, str)])\n",
    "        if N > 0:\n",
    "            for i, rle in enumerate(in_mask_list):\n",
    "                if not is_valid(rle):\n",
    "                    to_remove.append(idx)\n",
    "                    \n",
    "    for idx in to_remove:\n",
    "        del image_names[idx]\n",
    "        del image_masks[idx]\n",
    "    return image_names, image_masks\n",
    "        \n",
    "\n",
    "def get_train_valid_dfs(masks: dict, seed: int = 0) -> Tuple[list, list, list, list]:\n",
    "    ids = np.array(list(masks.keys())).reshape((len(masks),1))\n",
    "    train_ids, valid_ids = train_test_split(\n",
    "         ids, \n",
    "         test_size = 0.01, \n",
    "         random_state=seed\n",
    "        )\n",
    "    train_ids, valid_ids = list(train_ids.flatten()), list(valid_ids.flatten())\n",
    "    train_masks = [masks[idx] for idx in train_ids]\n",
    "    valid_masks = [masks[idx] for idx in valid_ids]\n",
    "    return train_ids, train_masks, valid_ids, valid_masks\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ship_dir = '../dev/'\n",
    "    train_image_dir = os.path.join(ship_dir, 'imgs/')\n",
    "    valid_image_dir = os.path.join(ship_dir, 'imgs/')\n",
    "    masks = get_masks(ship_dir, train_image_dir, valid_image_dir)\n",
    "    image_names, filtered_masks = filter_masks(masks)\n",
    "    train_ids, train_masks, valid_ids, valid_masks = get_train_valid_dfs(\n",
    "        filtered_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "190620"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_mask_list = train_masks[0]\n",
    "N = sum([1 for i in in_mask_list if isinstance(i, str)])\n",
    "\n",
    "make_target(in_mask_list, N, shape=(768, 768))['boxes'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([420]), array([1189998119991197253])]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = np.array([[1], [2]])\n",
    "print(list(ids.flatten()))\n",
    "masks = np.array([69, 420, 1189998119991197253])\n",
    "[masks[idx] for idx in ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.functional import resize\n",
    "\n",
    "\n",
    "class Resize:\n",
    "    def __init__(self, \n",
    "                 input_shape = (768, 768), \n",
    "                 output_shape = (299, 299), \n",
    "                 interpolation=2\n",
    "                ):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        self.interpolation = interpolation\n",
    "        \n",
    "        \n",
    "    def resize_boxes(self, boxes: torch.tensor) -> torch.tensor:\n",
    "        x_orig, y_orig = self.input_shape\n",
    "        x_new, y_new = self.output_shape\n",
    "        x_scale = x_new / x_orig\n",
    "        y_scale = y_new / y_orig\n",
    "        # bbox = tuple(x1, y1, x2, y2)\n",
    "        row_scaler = torch.tensor([x_scale, y_scale, x_scale, y_scale])\n",
    "        boxes_scaled = torch.round(boxes * row_scaler).int() # Converts to new coordinates\n",
    "        return boxes_scaled\n",
    "        \n",
    "        \n",
    "    def __call__(self, image, target) -> Tuple[torch.tensor, dict]:\n",
    "        image = resize(image, size=self.output_shape, interpolation=self.interpolation)\n",
    "        target['boxes'] = self.resize_boxes(target['boxes'])\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test passed\n"
     ]
    }
   ],
   "source": [
    "def test_resize():\n",
    "    rgb_path = r'../dev/imgs/0002756f7.jpg'\n",
    "    target = {}\n",
    "    target['boxes'] = torch.tensor([[100,100, 200, 200],[10,11,12,13],[14,15,16,17]])\n",
    "    image =  Image.open(rgb_path)\n",
    "    Resize()(image, target)\n",
    "    print(\"test passed\")\n",
    "\n",
    "test_resize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test `VesselDataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomBlur:\n",
    "    def __init__(self, p=0.5, radius=2):\n",
    "        self.p = p\n",
    "        self.radius = radius\n",
    "\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        prob = np.random.rand(1)[0]\n",
    "        if prob < self.p:\n",
    "            x = x.filter(ImageFilter.GaussianBlur(self.radius))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VesselDataset(Dataset):\n",
    "    def __init__(self, boxes: Optional[list], image_names: list, train_image_dir=None, valid_image_dir=None, \n",
    "                 test_image_dir=None, transform=None, mode='train', binary=True):\n",
    "        self.boxes = boxes\n",
    "        self.image_names = image_names\n",
    "        self.train_image_dir = train_image_dir\n",
    "        self.valid_image_dir = valid_image_dir\n",
    "        self.test_image_dir = test_image_dir\n",
    "\n",
    "        mean = [0.485, 0.456, 0.406]\n",
    "        std = [0.229, 0.224, 0.225]\n",
    "        if transform is not None:\n",
    "            self.train_transform = transform\n",
    "        else:\n",
    "            self.train_transform = Compose([\n",
    "                RandomBlur(p=0.95, radius=2),\n",
    "                ToTensor(),\n",
    "                Normalize(mean, std) # Apply to all input images\n",
    "            ])\n",
    "        self.valid_transform = Compose([\n",
    "            RandomBlur(p=1.0, radius=2), # Blur all images\n",
    "            ToTensor(),\n",
    "            Normalize(mean, std) # Apply to all input images\n",
    "        ])\n",
    "        self.test_transform = Compose([\n",
    "            transforms.Resize(size=(299,299), interpolation=2),\n",
    "            ToTensor(),\n",
    "            Normalize(mean, std) # Apply to all input images\n",
    "        ])\n",
    "        self.mode = mode\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.boxes)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_file_name = self.image_names[idx]\n",
    "        if self.mode == 'train':\n",
    "            img_path = os.path.join(self.train_image_dir, img_file_name)\n",
    "        elif self.mode == 'valid':\n",
    "            img_path = os.path.join(self.valid_image_dir, img_file_name)\n",
    "        else:\n",
    "            img_path = os.path.join(self.test_image_dir, img_file_name)\n",
    "\n",
    "        #img = imread(img_path)\n",
    "        img = Image.open(img_path)\n",
    "        if self.mode =='train' or self.mode =='valid':\n",
    "            img_boxes = self.boxes[idx]\n",
    "            N = sum([1 for i in img_boxes if isinstance(i, str)])\n",
    "            target = make_target(img_boxes, N, shape=(768, 768))\n",
    "            img, target = Resize(input_shape = (768, 768), \n",
    "                                 output_shape = (299, 299)\n",
    "                                )(img, target)\n",
    "        \n",
    "        if self.mode =='train':\n",
    "            img = self.train_transform(img)\n",
    "            return img, target\n",
    "        elif self.mode == 'valid':\n",
    "            img = self.valid_transform(img)\n",
    "            return img, target\n",
    "        else:\n",
    "            img = self.test_transform(img)\n",
    "            return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for ids, name in image_names.items():  # for name, age in dictionary.iteritems():  (for Python 2.x)\n",
    "    if name == r'000d26c17.jpg':\n",
    "        print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000d26c17.jpg\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 299, 299])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VesselDataset(train_masks, image_names, test_image_dir = r'../dev/imgs', mode='test').__getitem__(32).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Everything Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from typing import Callable, Iterator, Union, Optional, List, Tuple, Dict\n",
    "from torchvision.transforms.functional import resize\n",
    "\n",
    "\n",
    "def rle2bbox(rle, shape):\n",
    "    '''\n",
    "    rle: run-length encoded image mask, as string\n",
    "    shape: (height, width) of image on which RLE was produced\n",
    "    Returns (x0, y0, x1, y1) tuple describing the bounding box of the rle mask\n",
    "    \n",
    "    Note on image vs np.array dimensions:\n",
    "    \n",
    "        np.array implies the `[y, x]` indexing order in terms of image dimensions,\n",
    "        so the variable on `shape[0]` is `y`, and the variable on the `shape[1]` is `x`,\n",
    "        hence the result would be correct (x0,y0,x1,y1) in terms of image dimensions\n",
    "        for RLE-encoded indices of np.array (which are produced by widely used kernels\n",
    "        and are used in most kaggle competitions datasets)\n",
    "    '''\n",
    "    \n",
    "    a = np.fromiter(rle.split(), dtype=np.uint)\n",
    "    a = a.reshape((-1, 2))  # an array of (start, length) pairs\n",
    "    a[:,0] -= 1  # `start` is 1-indexed\n",
    "    \n",
    "    y0 = a[:,0] % shape[0]\n",
    "    y1 = y0 + a[:,1]\n",
    "    if np.any(y1 > shape[0]):\n",
    "        # got `y` overrun, meaning that there are a pixels in mask on 0 and shape[0] position\n",
    "        y0 = 0\n",
    "        y1 = shape[0]\n",
    "    else:\n",
    "        y0 = np.min(y0)\n",
    "        y1 = np.max(y1)\n",
    "    \n",
    "    x0 = a[:,0] // shape[0]\n",
    "    x1 = (a[:,0] + a[:,1]) // shape[0]\n",
    "    x0 = np.min(x0)\n",
    "    x1 = np.max(x1)\n",
    "    \n",
    "    if x1 > shape[1]:\n",
    "        # just went out of the image dimensions\n",
    "        raise ValueError(\"invalid RLE or image dimensions: x1=%d > shape[1]=%d\" % (\n",
    "            x1, shape[1]\n",
    "        ))\n",
    "\n",
    "    return x0, y0, x1, y1\n",
    "\n",
    "\n",
    "# From: https://www.kaggle.com/paulorzp/run-length-encode-and-decode\n",
    "def rle_decode(mask_rle, shape=(299, 768)):\n",
    "    '''\n",
    "    mask_rle: run-length as string formated (start length)\n",
    "    shape: (height,width) of array to return \n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "    '''\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape).T  # Needed to align to RLE direction\n",
    "\n",
    "\n",
    "def is_valid(rle, shape=(768,768)):\n",
    "    width, height = shape\n",
    "    xmin, ymin, xmax, ymax = rle2bbox(rle, shape)\n",
    "    if xmin >= 0 and xmax <= width and xmin < xmax and \\\n",
    "    ymin >= 0 and ymax <= height and ymin < ymax:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def make_target(in_mask_list, N, shape=(768, 768)):\n",
    "    if N == 0:\n",
    "        target = {}\n",
    "        target[\"boxes\"] = torch.zeros((0, 4), dtype=torch.float32)\n",
    "        target[\"labels\"] = torch.zeros((0), dtype=torch.int64)\n",
    "        return target\n",
    "    bbox_array = np.empty((N, 4), dtype=np.float32)\n",
    "    labels = torch.ones((N,), dtype=torch.int64)\n",
    "    i = 0\n",
    "    for rle in in_mask_list:\n",
    "        if isinstance(rle, str):\n",
    "            # bbox = tuple(x1, y1, x2, y2)\n",
    "            bbox = rle2bbox(rle, shape)\n",
    "            bbox_array[i,:] = bbox\n",
    "        i += 1\n",
    "    target = {\n",
    "        'boxes': torch.from_numpy(bbox_array),\n",
    "        'labels': labels,\n",
    "    }\n",
    "    return target\n",
    "\n",
    "\n",
    "def get_masks(ship_dir: str, \n",
    "                train_image_dir: Union[str, pathlib.Path], \n",
    "                valid_image_dir: Union[str, pathlib.Path]\n",
    "               ) -> pd.DataFrame:\n",
    "    masks = pd.read_csv(os.path.join(ship_dir,\n",
    "                                     'train_ship_segmentations_v2.csv'\n",
    "                                    )\n",
    "                       )\n",
    "    return masks\n",
    "\n",
    "\n",
    "def is_valid(rle, shape=(768,768)) -> bool:\n",
    "    width, height = shape\n",
    "    xmin, ymin, xmax, ymax = rle2bbox(rle, shape)\n",
    "    if xmin >= 0 and xmax <= width and xmin < xmax and \\\n",
    "    ymin >= 0 and ymax <= height and ymin < ymax:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def filter_masks(masks: pd.DataFrame) -> Tuple[dict, dict]:\n",
    "    grp = list(masks.groupby('ImageId'))\n",
    "    image_names =  {idx: filename for idx, (filename, _) in enumerate(grp)} \n",
    "    image_masks = {idx: m['EncodedPixels'].values for idx, (_, m) in enumerate(grp)}\n",
    "    to_remove = []\n",
    "    for idx, in_mask_list in image_masks.items():\n",
    "        N = sum([1 for i in in_mask_list if isinstance(i, str)])\n",
    "        if N > 0:\n",
    "            for i, rle in enumerate(in_mask_list):\n",
    "                if not is_valid(rle):\n",
    "                    to_remove.append(idx)\n",
    "                    \n",
    "    for idx in to_remove:\n",
    "        del image_names[idx]\n",
    "        del image_masks[idx]\n",
    "    return image_names, image_masks\n",
    "        \n",
    "\n",
    "def get_train_valid_dfs(masks: dict, seed: int = 0) -> Tuple[list, list, list, list]:\n",
    "    ids = np.array(list(masks.keys())).reshape((len(masks),1))\n",
    "    train_ids, valid_ids = train_test_split(\n",
    "         ids, \n",
    "         test_size = 0.01, \n",
    "         random_state=seed\n",
    "        )\n",
    "    train_ids, valid_ids = list(train_ids.flatten()), list(valid_ids.flatten())\n",
    "    train_masks = [masks[idx] for idx in train_ids]\n",
    "    valid_masks = [masks[idx] for idx in valid_ids]\n",
    "    return train_ids, train_masks, valid_ids, valid_masks\n",
    "\n",
    "\n",
    "class Resize:\n",
    "    def __init__(self, \n",
    "                 input_shape = (768, 768), \n",
    "                 output_shape = (299, 299), \n",
    "                 interpolation=2\n",
    "                ):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        self.interpolation = interpolation\n",
    "        \n",
    "        \n",
    "    def resize_boxes(self, boxes: torch.tensor) -> torch.tensor:\n",
    "        x_orig, y_orig = self.input_shape\n",
    "        x_new, y_new = self.output_shape\n",
    "        x_scale = x_new / x_orig\n",
    "        y_scale = y_new / y_orig\n",
    "        # bbox = tuple(x1, y1, x2, y2)\n",
    "        row_scaler = torch.tensor([x_scale, y_scale, x_scale, y_scale])\n",
    "        boxes_scaled = torch.round(boxes * row_scaler).int() # Converts to new coordinates\n",
    "        return boxes_scaled\n",
    "        \n",
    "        \n",
    "    def __call__(self, image, target) -> Tuple[torch.tensor, dict]:\n",
    "        image = resize(image, size=self.output_shape, interpolation=self.interpolation)\n",
    "        target['boxes'] = self.resize_boxes(target['boxes'])\n",
    "        return image, target\n",
    "    \n",
    "    \n",
    "class RandomBlur:\n",
    "    def __init__(self, p=0.5, radius=2):\n",
    "        self.p = p\n",
    "        self.radius = radius\n",
    "\n",
    "\n",
    "    def __call__(self, x):\n",
    "        prob = np.random.rand(1)[0]\n",
    "        if prob < self.p:\n",
    "            x = x.filter(ImageFilter.GaussianBlur(self.radius))\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class VesselDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 boxes: dict, \n",
    "                 image_ids: list,\n",
    "                 image_names: dict, \n",
    "                 train_image_dir=None, \n",
    "                 valid_image_dir=None, \n",
    "                 test_image_dir=None, \n",
    "                 transform=None, \n",
    "                 mode='train', \n",
    "                 binary=True):\n",
    "        self.boxes = boxes\n",
    "        self.image_ids = image_ids\n",
    "        self.image_names = image_names\n",
    "        self.train_image_dir = train_image_dir\n",
    "        self.valid_image_dir = valid_image_dir\n",
    "        self.test_image_dir = test_image_dir\n",
    "\n",
    "        mean = [0.485, 0.456, 0.406]\n",
    "        std = [0.229, 0.224, 0.225]\n",
    "        if transform is not None:\n",
    "            self.train_transform = transform\n",
    "        else:\n",
    "            self.train_transform = Compose([\n",
    "                RandomBlur(p=0.95, radius=2),\n",
    "                ToTensor(),\n",
    "                Normalize(mean, std) # Apply to all input images\n",
    "            ])\n",
    "        self.valid_transform = Compose([\n",
    "            RandomBlur(p=1.0, radius=2), # Blur all images\n",
    "            ToTensor(),\n",
    "            Normalize(mean, std) # Apply to all input images\n",
    "        ])\n",
    "        self.test_transform = Compose([\n",
    "            transforms.Resize(size=(299,299), interpolation=2),\n",
    "            ToTensor(),\n",
    "            Normalize(mean, std) # Apply to all input images\n",
    "        ])\n",
    "        self.mode = mode\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx = self.image_ids[idx] # Convert from input to image ID number\n",
    "        img_file_name = self.image_names[idx]\n",
    "        if self.mode == 'train':\n",
    "            img_path = os.path.join(self.train_image_dir, img_file_name)\n",
    "        elif self.mode == 'valid':\n",
    "            img_path = os.path.join(self.valid_image_dir, img_file_name)\n",
    "        else:\n",
    "            img_path = os.path.join(self.test_image_dir, img_file_name)\n",
    "\n",
    "        #img = imread(img_path)\n",
    "        img = Image.open(img_path)\n",
    "        if self.mode =='train' or self.mode =='valid':\n",
    "            img_boxes = self.boxes[idx]\n",
    "            N = sum([1 for i in img_boxes if isinstance(i, str)])\n",
    "            target = make_target(img_boxes, N, shape=(768, 768))\n",
    "            img, target = Resize(input_shape = (768, 768), \n",
    "                                 output_shape = (299, 299)\n",
    "                                )(img, target)\n",
    "            # Make image_id\n",
    "            image_id = torch.tensor([idx])\n",
    "            target[\"image_id\"] = image_id\n",
    "        \n",
    "        if self.mode =='train':\n",
    "            img = self.train_transform(img)\n",
    "            return img, target\n",
    "        elif self.mode == 'valid':\n",
    "            img = self.valid_transform(img)\n",
    "            return img, target\n",
    "        else:\n",
    "            img = self.test_transform(img)\n",
    "            return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transforms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-fdacc47eed2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mvessel_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVesselDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_image_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr'../dev/imgs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mvessel_valid_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVesselDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_image_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr'../dev/imgs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-9de2fea5b0cc>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, boxes, image_names, train_image_dir, valid_image_dir, test_image_dir, transform, mode, binary)\u001b[0m\n\u001b[1;32m    219\u001b[0m         ])\n\u001b[1;32m    220\u001b[0m         self.test_transform = Compose([\n\u001b[0;32m--> 221\u001b[0;31m             \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m299\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m299\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m             \u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mNormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Apply to all input images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'transforms' is not defined"
     ]
    }
   ],
   "source": [
    "ship_dir = '../dev/'\n",
    "train_image_dir = os.path.join(ship_dir, 'imgs/')\n",
    "valid_image_dir = os.path.join(ship_dir, 'imgs/')\n",
    "masks = get_masks(ship_dir, train_image_dir, valid_image_dir)\n",
    "image_names, filtered_masks = filter_masks(masks)\n",
    "train_ids, train_masks, valid_ids, valid_masks = get_train_valid_dfs(\n",
    "    filtered_masks\n",
    ")\n",
    "\n",
    "vessel_dataset = VesselDataset(train_masks, image_names, train_image_dir = r'../dev/imgs', mode='train')\n",
    "\n",
    "vessel_valid_dataset = VesselDataset(valid_masks, image_names, valid_image_dir = r'../dev/imgs', mode='valid')\n",
    "\n",
    "batch_size = 64\n",
    "shuffle = True\n",
    "loader = DataLoader(\n",
    "            dataset=vessel_dataset,\n",
    "            shuffle=shuffle,\n",
    "            #num_workers = 0,\n",
    "            batch_size=batch_size,\n",
    "            pin_memory=torch.cuda.is_available()\n",
    "        )\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "            dataset=vessel_valid_dataset,\n",
    "            shuffle=shuffle,\n",
    "            #num_workers = 0,\n",
    "            batch_size=batch_size,\n",
    "            pin_memory=torch.cuda.is_available()\n",
    "        )\n",
    "\n",
    "num_epochs = 30\n",
    "print_freq = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dummy Model and Test IO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://discuss.pytorch.org/t/faster-rcnn-with-inceptionv3-backbone-very-slow/91455\n",
    "def make_model(state_dict=None, num_classes=2):\n",
    "        inception = torchvision.models.inception_v3(pretrained=False, progress=False, \n",
    "                                                    num_classes=num_classes, aux_logits=False)\n",
    "        #inception.load_state_dict(torch.load(state_dict))\n",
    "        modules = list(inception.children())[:-1]\n",
    "        backbone = nn.Sequential(*modules)\n",
    "\n",
    "        for layer in backbone:\n",
    "            for p in layer.parameters():\n",
    "                p.requires_grad = False # Freezes the backbone layers\n",
    "\n",
    "        backbone.out_channels = 2048\n",
    "\n",
    "        anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "                                           aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "\n",
    "        model = FasterRCNN(backbone, rpn_anchor_generator=anchor_generator,\n",
    "                           box_predictor=FastRCNNPredictor(1024, num_classes))\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/poisson/lib/python3.7/site-packages/torchvision/models/inception.py:77: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
      "  ' due to scipy/scipy#11299), please set init_weights=True.', FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "model = make_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reformat `train_one_epoch` and `evaluate` For New Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp vision/references/detection/utils.py ./\n",
    "!cp vision/references/detection/transforms.py ./\n",
    "!cp vision/references/detection/coco_eval.py ./\n",
    "!cp vision/references/detection/engine.py ./\n",
    "!cp vision/references/detection/coco_utils.py ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pycocotools\n",
      "  Downloading pycocotools-2.0.2.tar.gz (23 kB)\n",
      "Requirement already satisfied: setuptools>=18.0 in /opt/miniconda3/envs/poisson/lib/python3.7/site-packages (from pycocotools) (47.3.0.post20200616)\n",
      "Collecting cython>=0.27.3\n",
      "  Using cached Cython-0.29.21-cp37-cp37m-macosx_10_9_x86_64.whl (1.9 MB)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in /opt/miniconda3/envs/poisson/lib/python3.7/site-packages (from pycocotools) (3.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/miniconda3/envs/poisson/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (0.10.0)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /opt/miniconda3/envs/poisson/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (2020.6.20)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/miniconda3/envs/poisson/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.15 in /opt/miniconda3/envs/poisson/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (1.18.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /opt/miniconda3/envs/poisson/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (2.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/miniconda3/envs/poisson/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (8.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/miniconda3/envs/poisson/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (2.8.1)\n",
      "Requirement already satisfied: six in /opt/miniconda3/envs/poisson/lib/python3.7/site-packages (from cycler>=0.10->matplotlib>=2.1.0->pycocotools) (1.15.0)\n",
      "Building wheels for collected packages: pycocotools\n",
      "  Building wheel for pycocotools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pycocotools: filename=pycocotools-2.0.2-cp37-cp37m-macosx_10_9_x86_64.whl size=90488 sha256=3d5863d3d1301f7eea1066a95f84b1df9912c97b06356d1e05afd3e7576ef152\n",
      "  Stored in directory: /Users/richardcorrero/Library/Caches/pip/wheels/bc/cf/1b/e95c99c5f9d1648be3f500ca55e7ce55f24818b0f48336adaf\n",
      "Successfully built pycocotools\n",
      "Installing collected packages: cython, pycocotools\n",
      "Successfully installed cython-0.29.21 pycocotools-2.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import torchvision.transforms as transforms\n",
    "from engine import train_one_epoch, evaluate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/poisson/lib/python3.7/site-packages/torchvision/models/inception.py:77: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
      "  ' due to scipy/scipy#11299), please set init_weights=True.', FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'error' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-e8f25b3cc3ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         train_one_epoch(model, optimizer, data_loader, device, epoch, lr_scheduler = None, \n\u001b[0m\u001b[1;32m     48\u001b[0m                         print_every = 100, num_epochs = 30)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_loader' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-e8f25b3cc3ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m         train_one_epoch(model, optimizer, data_loader, device, epoch, lr_scheduler = None, \n\u001b[1;32m     48\u001b[0m                         print_every = 100, num_epochs = 30)\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0;32mexcept\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'error' is not defined"
     ]
    }
   ],
   "source": [
    "ship_dir = '../dev/'\n",
    "train_image_dir = os.path.join(ship_dir, 'imgs/')\n",
    "valid_image_dir = os.path.join(ship_dir, 'imgs/')\n",
    "masks = get_masks(ship_dir, train_image_dir, valid_image_dir)\n",
    "image_names, filtered_masks = filter_masks(masks)\n",
    "train_ids, train_masks, valid_ids, valid_masks = get_train_valid_dfs(\n",
    "    filtered_masks\n",
    ")\n",
    "\n",
    "vessel_dataset = VesselDataset(train_masks, image_names, train_image_dir = r'../dev/imgs', mode='train')\n",
    "\n",
    "vessel_valid_dataset = VesselDataset(valid_masks, image_names, valid_image_dir = r'../dev/imgs', mode='valid')\n",
    "\n",
    "batch_size = 64\n",
    "shuffle = True\n",
    "loader = DataLoader(\n",
    "            dataset=vessel_dataset,\n",
    "            shuffle=shuffle,\n",
    "            #num_workers = 0,\n",
    "            batch_size=batch_size,\n",
    "            pin_memory=torch.cuda.is_available()\n",
    "        )\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "            dataset=vessel_valid_dataset,\n",
    "            shuffle=shuffle,\n",
    "            #num_workers = 0,\n",
    "            batch_size=batch_size,\n",
    "            pin_memory=torch.cuda.is_available()\n",
    "        )\n",
    "\n",
    "model = make_model()\n",
    "\n",
    "num_epochs = 30\n",
    "print_freq = 100\n",
    "\n",
    "device = torch.device('cpu')\n",
    "model = model.to(device)\n",
    "savepath = r'./sdad'\n",
    "lr = 1e-4\n",
    "weight_decay = 1e-7 # Default should be 1e-5\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "print('Starting Training...\\n')\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "    try:\n",
    "        train_one_epoch(model, optimizer, data_loader, device, epoch, lr_scheduler = None, \n",
    "                        print_every = 100, num_epochs = 30)\n",
    "    except:\n",
    "        continue\n",
    "    print('Epoch %d completed. Running validation...\\n' % (epoch + 1))\n",
    "    try:\n",
    "        metrics = evaluate(model, data_loader, device)\n",
    "    except error:\n",
    "        continue\n",
    "    print('Saving Model...\\n')\n",
    "    torch.save(model.state_dict(), savepath)\n",
    "    print('Model Saved.\\n')\n",
    "print('Finished Training.\\n')\n",
    "print('Saving Model...\\n')\n",
    "torch.save(model.state_dict(), savepath)\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rewrite Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_print(running_loss, \n",
    "                print_every, \n",
    "                batch_size, \n",
    "                epoch, \n",
    "                num_minibatches_per_epoch, \n",
    "                time_left):\n",
    "    print('[%d, %5d] Running Loss: %.3f' %\n",
    "          (epoch + 1, i + 1, (running_loss / print_every)))\n",
    "    print('           Number of Samples Seen: %d' %\n",
    "          (batch_size * ((i + 1) + epoch * num_minibatches_per_epoch)))\n",
    "    print('           Estimated Hours Remaining: %.2f\\n' % time_left)\n",
    "\n",
    "\n",
    "def train_one_epoch(model, \n",
    "                    optimizer, \n",
    "                    data_loader, \n",
    "                    device, \n",
    "                    epoch, \n",
    "                    lr_scheduler = None, \n",
    "                    print_every = 100,\n",
    "                    num_epochs = 30):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    minibatch_time = 0.0\n",
    "#    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
    "#    header = 'Epoch: [{}]'.format(epoch)\n",
    "#\n",
    "#    lr_scheduler = None\n",
    "#    if epoch == 0:\n",
    "#        warmup_factor = 1. / 1000\n",
    "#        warmup_iters = min(1000, len(data_loader) - 1)\n",
    "#\n",
    "#        lr_scheduler = utils.warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor)\n",
    "\n",
    "    for i, (inputs, targets) in enumerate(data_loader):\n",
    "        start = time.time()\n",
    "#        images = list(image.to(device) for image in images)\n",
    "        inputs = Variable(inputs).cuda()\n",
    "        targets = [{k: Variable(v).cuda() for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(inputs, targets)\n",
    "\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        #        # reduce losses over all GPUs for logging purposes\n",
    "        #        loss_dict_reduced = utils.reduce_dict(loss_dict)\n",
    "        #        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
    "\n",
    "        #        loss_value = losses_reduced.item()\n",
    "\n",
    "        if not math.isfinite(losses):\n",
    "            print(\"Loss is %-10.5f, stopping training\".format(losses))\n",
    "            sys.exit(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        end = time.time()\n",
    "        minibatch_time += float(end - start)\n",
    "        if (i + 1) % print_every == 0:\n",
    "            minibatch_time = minibatch_time / (3600.0 * print_every)\n",
    "            num_minibatches_left = 1.01 * len(data_loader) - (i + 1)\n",
    "            num_minibatches_per_epoch = 1.01 * len(data_loader) - 1 + \\\n",
    "            ((len(dataloader.dataset) % batch_size) / batch_size)\n",
    "            num_epochs_left = num_epochs - (epoch + 1)\n",
    "            time_left = minibatch_time * \\\n",
    "                (num_minibatches_left + num_epochs_left * num_minibatches_per_epoch)\n",
    "            time_left *= 6.0 # Adjust for timing discrepencies\n",
    "            train_print(running_loss, print_every, batch_size, epoch, \n",
    "                        num_minibatches_per_epoch, time_left)\n",
    "            running_loss = 0.0\n",
    "            minibatch_time = 0.0\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data_loader):\n",
    "    n_threads = torch.get_num_threads()\n",
    "    torch.set_num_threads(n_threads)\n",
    "    cpu_device = torch.device(\"cpu\")\n",
    "    model.eval()\n",
    "\n",
    "    coco = get_coco_api_from_dataset(data_loader.dataset)\n",
    "    iou_types = _get_iou_types(model)\n",
    "    coco_evaluator = CocoEvaluator(coco, iou_types)\n",
    "\n",
    "    for images, targets in metric_logger.log_every(data_loader, 100, header):\n",
    "        images = list(Variable(img).to(device) for img in images)\n",
    "\n",
    "        model_time = time.time()\n",
    "        outputs = model(images)\n",
    "\n",
    "        outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
    "        model_time = time.time() - model_time\n",
    "\n",
    "        res = {target[\"image_id\"].item(): output for target, output in zip(targets, outputs)}\n",
    "        evaluator_time = time.time()\n",
    "        coco_evaluator.update(res)\n",
    "        evaluator_time = time.time() - evaluator_time\n",
    "        metric_logger.update(model_time=model_time, evaluator_time=evaluator_time)\n",
    "\n",
    "    # gather the stats from all processes\n",
    "    metric_logger.synchronize_between_processes()\n",
    "    print(\"Averaged stats:\", metric_logger)\n",
    "    coco_evaluator.synchronize_between_processes()\n",
    "\n",
    "    # accumulate predictions from all images\n",
    "    coco_evaluator.accumulate()\n",
    "    coco_evaluator.summarize()\n",
    "    torch.set_num_threads(n_threads)\n",
    "    return coco_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Starting Training...\\n')\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, lr_scheduler = None, \n",
    "                    print_every = 100, num_epochs = 30)\n",
    "    print('Epoch %d completed. Running validation...\\n' % (epoch + 1))\n",
    "    metrics = evaluate(model, data_loader, device)\n",
    "    print('Saving Model...\\n')\n",
    "    torch.save(model.state_dict(), savepath)\n",
    "    print('Model Saved.\\n')\n",
    "print('Finished Training.\\n')\n",
    "print('Saving Model...\\n')\n",
    "torch.save(model.state_dict(), savepath)\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/poisson/lib/python3.7/site-packages/torchvision/models/inception.py:77: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
      "  ' due to scipy/scipy#11299), please set init_weights=True.', FutureWarning)\n",
      "Train Size: 190620\n",
      "Valid Size: 1926\n",
      "Starting Training...\n",
      "\n",
      "Epoch 1 completed. Running validation...\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"../vessel_detector_test_suite.py\", line 494, in <module>\n",
      "    main()\n",
      "  File \"../vessel_detector_test_suite.py\", line 481, in main\n",
      "    metrics = evaluate(model, valid_loader, device)\n",
      "  File \"/opt/miniconda3/envs/poisson/lib/python3.7/site-packages/torch/autograd/grad_mode.py\", line 26, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"../vessel_detector_test_suite.py\", line 385, in evaluate\n",
      "    coco = get_coco_api_from_dataset(data_loader.dataset)\n",
      "  File \"/Users/richardcorrero/Projects/research/poisson/coco_utils.py\", line 206, in get_coco_api_from_dataset\n",
      "    return convert_to_coco_api(dataset)\n",
      "  File \"/Users/richardcorrero/Projects/research/poisson/coco_utils.py\", line 155, in convert_to_coco_api\n",
      "    img, targets = ds[img_idx]\n",
      "  File \"../vessel_detector_test_suite.py\", line 270, in __getitem__\n",
      "    img = Image.open(img_path)\n",
      "  File \"/opt/miniconda3/envs/poisson/lib/python3.7/site-packages/PIL/Image.py\", line 2891, in open\n",
      "    fp = builtins.open(filename, \"rb\")\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '../dev/imgs/64d82cd21.jpg'\n"
     ]
    }
   ],
   "source": [
    "! python ../vessel_detector_test_suite.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resize:\n",
    "    def __init__(self, \n",
    "                 input_shape = (768, 768), \n",
    "                 output_shape = (299, 299), \n",
    "                 interpolation=2\n",
    "                ):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        self.interpolation = interpolation\n",
    "        \n",
    "        \n",
    "    def resize_boxes(self, boxes: torch.tensor) -> torch.tensor:\n",
    "        x_orig, y_orig = self.input_shape\n",
    "        x_new, y_new = self.output_shape\n",
    "        x_scale = x_new / x_orig\n",
    "        y_scale = y_new / y_orig\n",
    "        # bbox = tuple(x1, y1, x2, y2)\n",
    "        row_scaler = torch.tensor([x_scale, y_scale, x_scale, y_scale])\n",
    "        boxes_scaled = torch.round(boxes * row_scaler).int() # Converts to new coordinates\n",
    "        return boxes_scaled\n",
    "        \n",
    "        \n",
    "    def __call__(self, image, target) -> Tuple[torch.tensor, dict]:\n",
    "        image = resize(image, size=self.output_shape, interpolation=self.interpolation)\n",
    "        target['masks'] = resize(target['masks'], size=self.output_shape,\n",
    "                                interpolation=self.interpolation)\n",
    "        target['boxes'] = self.resize_boxes(target['boxes'])\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_target(in_mask_list=None, N=0, shape=(768, 768)):\n",
    "    if N == 0:\n",
    "        target = {}\n",
    "        target[\"boxes\"] = torch.zeros((0, 4), dtype=torch.float32)\n",
    "        target[\"labels\"] = torch.zeros((0), dtype=torch.int64)\n",
    "        target[\"masks\"] = torch.from_numpy(np.zeros((1, shape[0], shape[1]), dtype=np.uint8)) \n",
    "        target[\"area\"] = torch.zeros((0,), dtype=torch.int64)\n",
    "        target[\"iscrowd\"] = torch.zeros((0,), dtype=torch.int64)\n",
    "        return target\n",
    "    bbox_array = np.zeros((N, 4), dtype=np.float32)\n",
    "    masks = np.zeros((N, shape[0], shape[1]), dtype=np.uint8)\n",
    "    labels = torch.ones((N,), dtype=torch.int64)\n",
    "    i = 0\n",
    "    for rle in in_mask_list:\n",
    "        if isinstance(rle, str):\n",
    "        # bbox = tuple(x1, y1, x2, y2)\n",
    "            bbox = rle2bbox(rle, shape)\n",
    "            bbox_array[i,:] = bbox\n",
    "            mask = rle_decode(rle)\n",
    "            masks[i, :, :] = mask\n",
    "            i += 1\n",
    "    areas = (bbox_array[:, 3] - bbox_array[:, 1]) * (bbox_array[:, 2] - bbox_array[:, 0])\n",
    "    # suppose all instances are not crowd\n",
    "    is_crowd = torch.zeros((N,), dtype=torch.int64)\n",
    "    target = {\n",
    "        'boxes': torch.from_numpy(bbox_array),\n",
    "        'labels': labels,\n",
    "        'masks': torch.from_numpy(masks),\n",
    "        'area': torch.from_numpy(areas),\n",
    "        'iscrowd': is_crowd\n",
    "    }\n",
    "    return target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    loss_value = 0.0\n",
    "\n",
    "    for images, targets in data_loader:\n",
    "        images = list(img.to(device) for img in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            loss_dict = model(images)\n",
    "\n",
    "            # this returned object from the model:\n",
    "            # len is 4 (so index here), which is probably because of the size of the batch\n",
    "            # loss_dict[index]['boxes']\n",
    "            # loss_dict[index]['labels']\n",
    "            # loss_dict[index]['scores']\n",
    "            for x in range(image_batch_size):\n",
    "                loss_value += sum(loss for loss in loss_dict[x]['scores'])\n",
    "                \n",
    "        running_loss += loss_value\n",
    "        \n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8),\n",
       " {'boxes': tensor([], size=(0, 4), dtype=torch.int32),\n",
       "  'labels': tensor([], dtype=torch.int64),\n",
       "  'masks': tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           ...,\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8),\n",
       "  'area': tensor([], dtype=torch.int64),\n",
       "  'iscrowd': tensor([], dtype=torch.int64)})"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = make_target()\n",
    "Resize()(target['masks'], target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the predicted boxes (and scores) from my locally trained model.\n",
    "preds = torch.tensor([[956, 409, 68, 85],\n",
    "                  [883, 945, 85, 77],\n",
    "                  [745, 468, 81, 87],\n",
    "                  [658, 239, 103, 105],\n",
    "                  [518, 419, 91, 100],\n",
    "                  [711, 805, 92, 106],\n",
    "                  [62, 213, 72, 64],\n",
    "                  [884, 175, 109, 68],\n",
    "                  [721, 626, 96, 104],\n",
    "                  [878, 619, 121, 81],\n",
    "                  [887, 107, 111, 71],\n",
    "                  [827, 525, 88, 83],\n",
    "                  [816, 868, 102, 86],\n",
    "                  [166, 882, 78, 75],\n",
    "                  [603, 563, 78, 97],\n",
    "                  [744, 916, 68, 52],\n",
    "                  [582, 86, 86, 72],\n",
    "                  [79, 715, 91, 101],\n",
    "                  [246, 586, 95, 80],\n",
    "                  [181, 512, 93, 89],\n",
    "                  [655, 527, 99, 90],\n",
    "                  [568, 363, 61, 76],\n",
    "                  [9, 717, 152, 110],\n",
    "                  [576, 698, 75, 78],\n",
    "                  [805, 974, 75, 50],\n",
    "                  [10, 15, 78, 64],\n",
    "                  [826, 40, 69, 74],\n",
    "                  [32, 983, 106, 40]]).float()\n",
    "\n",
    "targs = torch.tensor([[954, 391,  70,  90],\n",
    "       [660, 220,  95, 102],\n",
    "       [ 64, 209,  76,  57],\n",
    "       [896,  99, 102,  69],\n",
    "       [747, 460,  72,  77],\n",
    "       [885, 163, 103,  69],\n",
    "       [514, 399,  90,  97],\n",
    "       [702, 794,  97,  99],\n",
    "       [721, 624,  98, 108],\n",
    "       [826, 512,  82,  94],\n",
    "       [883, 944,  79,  74],\n",
    "       [247, 594, 123,  92],\n",
    "       [673, 514,  95, 113],\n",
    "       [829, 847, 102, 110],\n",
    "       [ 94, 737,  92, 107],\n",
    "       [588, 568,  75, 107],\n",
    "       [158, 890, 103,  64],\n",
    "       [744, 906,  75,  79],\n",
    "       [826,  33,  72,  74],\n",
    "       [601,  69,  67,  87]]).float()\n",
    "\n",
    "scores = torch.tensor([0.9932319, 0.99206185, 0.99145633, 0.9898089, 0.98906296, 0.9817738,\n",
    "                   0.9799762, 0.97967803, 0.9771589, 0.97688967, 0.9562935, 0.9423076,\n",
    "                   0.93556845, 0.9236257, 0.9102379, 0.88644403, 0.8808225, 0.85238415,\n",
    "                   0.8472188, 0.8417798, 0.79908705, 0.7963756, 0.7437897, 0.6044758,\n",
    "                   0.59249884, 0.5557045, 0.53130984, 0.5020239])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0002756f7.jpg 0005d01c8.jpg 000d26c17.jpg 000f7e728.jpg 0017c19d6.jpg\r\n",
      "000592296.jpg 0006c52e8.jpg 000d42241.jpg 0014b1235.jpg\r\n"
     ]
    }
   ],
   "source": [
    "! ls '../dev/imgs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google_creds.json               train_ship_segmentations_v2.csv\r\n",
      "\u001b[34mimgs\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "! ls '../dev/train_ship_segmentations_v2.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>EncodedPixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00003e153.jpg</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001124c7.jpg</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000155de5.jpg</td>\n",
       "      <td>264661 17 265429 33 266197 33 266965 33 267733...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000194a2d.jpg</td>\n",
       "      <td>360486 1 361252 4 362019 5 362785 8 363552 10 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000194a2d.jpg</td>\n",
       "      <td>51834 9 52602 9 53370 9 54138 9 54906 9 55674 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ImageId                                      EncodedPixels\n",
       "0  00003e153.jpg                                                NaN\n",
       "1  0001124c7.jpg                                                NaN\n",
       "2  000155de5.jpg  264661 17 265429 33 266197 33 266965 33 267733...\n",
       "3  000194a2d.jpg  360486 1 361252 4 362019 5 362785 8 363552 10 ...\n",
       "4  000194a2d.jpg  51834 9 52602 9 53370 9 54138 9 54906 9 55674 ..."
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../dev/train_ship_segmentations_v2.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names, image_masks = filter_masks(df[df['ImageId'] == '0002756f7.jpg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['255784 2 256552 4 257319 7 258087 9 258854 12 259622 14 260389 16 261159 16 261929 16 262699 16 263469 16 264239 16 265009 14 265779 11 266549 9 267319 6 268089 4 268859 1',\n",
       "        '248878 1 249645 4 250413 6 251180 9 251948 10 252715 13 253482 16 254250 18 255019 18 255789 18 256559 18 257329 18 258099 17 258869 15 259639 12 260409 10 261179 7 261949 5 262719 2 263488 1 264255 1']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_masks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_target(in_mask_list, N, shape=(768, 768), null_mask_shape=(299,299)):\n",
    "    if N == 0:\n",
    "        target = {}\n",
    "        target[\"boxes\"] = torch.zeros((0, 4), dtype=torch.float32)\n",
    "        target[\"labels\"] = torch.zeros((0), dtype=torch.int64)\n",
    "        #target[\"masks\"] = torch.from_numpy(np.zeros((1,\n",
    "        #                                             null_mask_shape[0],\n",
    "        #                                             null_mask_shape[1]),\n",
    "        #                                             dtype=np.uint8)) \n",
    "        target[\"area\"] = torch.zeros((0,), dtype=torch.int64)\n",
    "        target[\"iscrowd\"] = torch.zeros((0,), dtype=torch.int64)\n",
    "        return target\n",
    "    bbox_array = np.zeros((N, 4), dtype=np.float32)\n",
    "    #masks = np.zeros((N, shape[0], shape[1]), dtype=np.uint8)\n",
    "    labels = torch.ones((N,), dtype=torch.int64)\n",
    "    i = 0\n",
    "    for rle in in_mask_list:\n",
    "        if isinstance(rle, str):\n",
    "            # bbox = tuple(x1, y1, x2, y2)\n",
    "            bbox = rle2bbox(rle, shape)\n",
    "            bbox_array[i,:] = bbox\n",
    "            i += 1\n",
    "    areas = (bbox_array[:, 3] - bbox_array[:, 1]) * (bbox_array[:, 2] - bbox_array[:, 0])\n",
    "    # suppose all instances are not crowd\n",
    "    is_crowd = torch.zeros((N,), dtype=torch.int64)\n",
    "    target = {\n",
    "        'boxes': torch.from_numpy(bbox_array),\n",
    "        'labels': labels,\n",
    "        #'masks': torch.from_numpy(masks),\n",
    "        'area': torch.from_numpy(areas),\n",
    "        'iscrowd': is_crowd\n",
    "    }\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boxes': tensor([[333.,  36., 350.,  62.],\n",
       "         [324.,  41., 344.,  67.]]),\n",
       " 'labels': tensor([1, 1]),\n",
       " 'area': tensor([442., 520.]),\n",
       " 'iscrowd': tensor([0, 0])}"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_boxes = image_masks[0]\n",
    "N = sum([1 for i in img_boxes if isinstance(i, str)])\n",
    "target = make_target(img_boxes, N, shape=(768, 768))\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://discuss.pytorch.org/t/faster-rcnn-with-inceptionv3-backbone-very-slow/91455\n",
    "def make_model(state_dict=None, num_classes=2):\n",
    "        inception = torchvision.models.inception_v3(pretrained=True, progress=False, \n",
    "                                                    num_classes=1000, aux_logits=False)\n",
    "        #inception.load_state_dict(torch.load(state_dict))\n",
    "        modules = list(inception.children())[:-1]\n",
    "        backbone = nn.Sequential(*modules)\n",
    "\n",
    "        for layer in backbone:\n",
    "            for p in layer.parameters():\n",
    "                p.requires_grad = False # Freezes the backbone layers\n",
    "\n",
    "        backbone.out_channels = 2048\n",
    "\n",
    "        anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "                                           aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "\n",
    "        model = FasterRCNN(backbone, rpn_anchor_generator=anchor_generator,\n",
    "                           box_predictor=FastRCNNPredictor(1024, num_classes))\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3333, 0.3255, 0.3137,  ..., 0.2902, 0.3216, 0.3137],\n",
      "         [0.3137, 0.3137, 0.3059,  ..., 0.2863, 0.2706, 0.2824],\n",
      "         [0.3137, 0.2980, 0.3059,  ..., 0.3451, 0.3569, 0.3686],\n",
      "         ...,\n",
      "         [0.4510, 0.4588, 0.5255,  ..., 0.3373, 0.3333, 0.3098],\n",
      "         [0.4353, 0.4824, 0.5412,  ..., 0.2902, 0.3216, 0.2824],\n",
      "         [0.4471, 0.4902, 0.5490,  ..., 0.3059, 0.3216, 0.3020]],\n",
      "\n",
      "        [[0.4510, 0.4431, 0.4314,  ..., 0.4039, 0.4353, 0.4275],\n",
      "         [0.4314, 0.4314, 0.4235,  ..., 0.4000, 0.3843, 0.3961],\n",
      "         [0.4314, 0.4157, 0.4235,  ..., 0.4588, 0.4706, 0.4824],\n",
      "         ...,\n",
      "         [0.4667, 0.4667, 0.5294,  ..., 0.4549, 0.4510, 0.4275],\n",
      "         [0.4471, 0.4902, 0.5451,  ..., 0.4078, 0.4392, 0.4000],\n",
      "         [0.4588, 0.4980, 0.5529,  ..., 0.4235, 0.4392, 0.4196]],\n",
      "\n",
      "        [[0.4196, 0.4118, 0.4000,  ..., 0.3843, 0.4157, 0.4078],\n",
      "         [0.4000, 0.4000, 0.3922,  ..., 0.3804, 0.3647, 0.3765],\n",
      "         [0.4000, 0.3843, 0.3922,  ..., 0.4392, 0.4510, 0.4627],\n",
      "         ...,\n",
      "         [0.4078, 0.4039, 0.4667,  ..., 0.4235, 0.4196, 0.3961],\n",
      "         [0.3922, 0.4275, 0.4824,  ..., 0.3765, 0.4078, 0.3686],\n",
      "         [0.4000, 0.4353, 0.4902,  ..., 0.3922, 0.4078, 0.3882]]])\n"
     ]
    }
   ],
   "source": [
    "img_path = r'../dev/imgs/0002756f7.jpg'\n",
    "img = Image.open(img_path)\n",
    "test_transform = Compose([\n",
    "            transforms.Resize(size=(299,299), interpolation=2),\n",
    "            ToTensor(),\n",
    "        ])\n",
    "img_ = test_transform(img)\n",
    "print(img_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to('cpu')\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    img = [img_.to('cpu')]  \n",
    "    outputs = model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000, 97.0355, 99.5819],\n",
       "        [ 0.0000,  0.0000, 35.3034, 18.7819],\n",
       "        [ 0.0000,  0.0000,  7.3229, 16.1216],\n",
       "        [ 0.0000,  0.0000, 15.7531, 30.9338],\n",
       "        [ 0.0000,  0.0000,  6.7129,  6.1982],\n",
       "        [ 0.0000,  0.0000, 66.9045, 30.3813],\n",
       "        [ 0.0000,  0.0000, 17.0096,  9.4770],\n",
       "        [ 0.0000,  0.0000, 32.2110, 73.5183]])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0]['boxes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5010, 0.5010, 0.5010, 0.5010, 0.5010, 0.5010, 0.5010, 0.5010])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0]['scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[333.,  36., 350.,  62.],\n",
       "        [324.,  41., 344.,  67.]])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target['boxes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import tensor\n",
    "from torchvision.ops.boxes import box_iou\n",
    "\n",
    "def align_coordinates(boxes):\n",
    "    \"\"\"Align coordinates (x1,y1) < (x2,y2) to work with torchvision `box_iou` op\n",
    "    Arguments:\n",
    "        boxes (Tensor[N,4])\n",
    "    \n",
    "    Returns:\n",
    "        boxes (Tensor[N,4]): aligned box coordinates\n",
    "    \"\"\"\n",
    "    x1y1 = torch.min(boxes[:,:2,],boxes[:, 2:])\n",
    "    x2y2 = torch.max(boxes[:,:2,],boxes[:, 2:])\n",
    "    boxes = torch.cat([x1y1,x2y2],dim=1)\n",
    "    return boxes\n",
    "\n",
    "\n",
    "def calculate_iou(gt, pr, form='pascal_voc'):\n",
    "    \"\"\"Calculates the Intersection over Union.\n",
    "\n",
    "    Arguments:\n",
    "        gt: (torch.Tensor[N,4]) coordinates of the ground-truth boxes\n",
    "        pr: (torch.Tensor[M,4]) coordinates of the prdicted boxes\n",
    "        form: (str) gt/pred coordinates format\n",
    "            - pascal_voc: [xmin, ymin, xmax, ymax]\n",
    "            - coco: [xmin, ymin, w, h]\n",
    "    Returns:\n",
    "        iou (Tensor[N, M]): the NxM matrix containing the pairwise\n",
    "        IoU values for every element in boxes1 and boxes2\n",
    "    \"\"\"\n",
    "    if form == 'coco':\n",
    "        gt = gt.clone()\n",
    "        pr = pr.clone()\n",
    "\n",
    "        gt[:,2] = gt[:,0] + gt[:,2]\n",
    "        gt[:,3] = gt[:,1] + gt[:,3]\n",
    "        pr[:,2] = pr[:,0] + pr[:,2]\n",
    "        pr[:,3] = pr[:,1] + pr[:,3]\n",
    "\n",
    "    gt = align_coordinates(gt)\n",
    "    pr = align_coordinates(pr)\n",
    "    \n",
    "    return box_iou(gt,pr)\n",
    "\n",
    "\n",
    "def get_mappings(iou_mat):\n",
    "    mappings = torch.zeros_like(iou_mat)\n",
    "    gt_count, pr_count = iou_mat.shape\n",
    "    \n",
    "    #first mapping (max iou for first pred_box)\n",
    "    if not iou_mat[:,0].eq(0.).all():\n",
    "        # if not a zero column\n",
    "        mappings[iou_mat[:,0].argsort()[-1],0] = 1\n",
    "\n",
    "    for pr_idx in range(1,pr_count):\n",
    "        # Sum of all the previous mapping columns will let \n",
    "        # us know which gt-boxes are already assigned\n",
    "        not_assigned = torch.logical_not(mappings[:,:pr_idx].sum(1)).long()\n",
    "\n",
    "        # Considering unassigned gt-boxes for further evaluation \n",
    "        targets = not_assigned * iou_mat[:,pr_idx]\n",
    "\n",
    "        # If no gt-box satisfy the previous conditions\n",
    "        # for the current pred-box, ignore it (False Positive)\n",
    "        if targets.eq(0).all():\n",
    "            continue\n",
    "\n",
    "        # max-iou from current column after all the filtering\n",
    "        # will be the pivot element for mapping\n",
    "        pivot = targets.argsort()[-1]\n",
    "        mappings[pivot,pr_idx] = 1\n",
    "    return mappings\n",
    "\n",
    "\n",
    "def calculate_map(gt_boxes,pr_boxes,scores,thresh=0.5,form='pascal_voc'):\n",
    "    # sorting\n",
    "    pr_boxes = pr_boxes[scores.argsort().flip(-1)]\n",
    "    iou_mat = calculate_iou(gt_boxes,pr_boxes,form) \n",
    "    \n",
    "    # thresholding\n",
    "    iou_mat = iou_mat.where(iou_mat>thresh,tensor(0.))\n",
    "    \n",
    "    mappings = get_mappings(iou_mat)\n",
    "    \n",
    "    # mAP calculation\n",
    "    tp = mappings.sum()\n",
    "    fp = mappings.sum(0).eq(0).sum()\n",
    "    fn = mappings.sum(1).eq(0).sum()\n",
    "    mAP = tp / (tp+fp+fn)\n",
    "    \n",
    "    return mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_map(target['boxes'],outputs[0]['boxes'],outputs[0]['scores'],form='pascal_voc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, data_loader, device, thresh_list = [0.25, 0.5, 0.75, 0.9]):\n",
    "#    n_threads = torch.get_num_threads()\n",
    "#    torch.set_num_threads(n_threads)\n",
    "    cpu_device = torch.device(\"cpu\")\n",
    "    model.eval()\n",
    "\n",
    "    #coco = get_coco_api_from_dataset(data_loader.dataset)\n",
    "    #iou_types = _get_iou_types(model)\n",
    "    #coco_evaluator = CocoEvaluator(coco, iou_types)\n",
    "    start = time.time()\n",
    "    mAP_dict = {thresh: [] for thresh in thresh_list}\n",
    "    for images, targets in data_loader:\n",
    "        images = list(Variable(img).to(device) for img in images)\n",
    "        targets = [{k: Variable(v).to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "#        model_time = time.time()\n",
    "        outputs = model(images, targets)\n",
    "        outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
    "#        model_time = time.time() - model_time\n",
    "#        res = {target[\"image_id\"].item(): output for target, output in zip(targets, outputs)}\n",
    "        # Calculate mAP\n",
    "        for thresh in thresh_list:\n",
    "            mAP_list = [calculate_map(target['boxes'], \n",
    "                                      output['boxes'], \n",
    "                                      output['scores'], \n",
    "                                      thresh=thresh) \\\n",
    "                        for target, output in zip(targets, outputs)]\n",
    "            #mAP = np.mean(mAP_list)\n",
    "            mAP_dict[thresh] += mAP_list # Creates a list of mAP's for each sample\n",
    "    #n_samples = len(data_loader.dataset)\n",
    "    end = time.time()\n",
    "    for thresh in thresh_list:\n",
    "        mAP_dict[thresh] = np.mean(mAP_dict[thresh])\n",
    "    # Create metrics dict\n",
    "    metrics = mAP_dict\n",
    "    metrics['eval_time'] = end - start\n",
    "        \n",
    "\n",
    "    # gather the stats from all processes\n",
    "#    metric_logger.synchronize_between_processes()\n",
    "#    print(\"Averaged stats:\", metric_logger)\n",
    "#    coco_evaluator.synchronize_between_processes()\n",
    "\n",
    "    # accumulate predictions from all images\n",
    " #   coco_evaluator.accumulate()\n",
    " #   coco_evaluator.summarize()\n",
    " #   torch.set_num_threads(n_threads)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = [([img_.to('cpu')], [target])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([tensor([[[0.3333, 0.3255, 0.3137,  ..., 0.2902, 0.3216, 0.3137],\n",
       "            [0.3137, 0.3137, 0.3059,  ..., 0.2863, 0.2706, 0.2824],\n",
       "            [0.3137, 0.2980, 0.3059,  ..., 0.3451, 0.3569, 0.3686],\n",
       "            ...,\n",
       "            [0.4510, 0.4588, 0.5255,  ..., 0.3373, 0.3333, 0.3098],\n",
       "            [0.4353, 0.4824, 0.5412,  ..., 0.2902, 0.3216, 0.2824],\n",
       "            [0.4471, 0.4902, 0.5490,  ..., 0.3059, 0.3216, 0.3020]],\n",
       "   \n",
       "           [[0.4510, 0.4431, 0.4314,  ..., 0.4039, 0.4353, 0.4275],\n",
       "            [0.4314, 0.4314, 0.4235,  ..., 0.4000, 0.3843, 0.3961],\n",
       "            [0.4314, 0.4157, 0.4235,  ..., 0.4588, 0.4706, 0.4824],\n",
       "            ...,\n",
       "            [0.4667, 0.4667, 0.5294,  ..., 0.4549, 0.4510, 0.4275],\n",
       "            [0.4471, 0.4902, 0.5451,  ..., 0.4078, 0.4392, 0.4000],\n",
       "            [0.4588, 0.4980, 0.5529,  ..., 0.4235, 0.4392, 0.4196]],\n",
       "   \n",
       "           [[0.4196, 0.4118, 0.4000,  ..., 0.3843, 0.4157, 0.4078],\n",
       "            [0.4000, 0.4000, 0.3922,  ..., 0.3804, 0.3647, 0.3765],\n",
       "            [0.4000, 0.3843, 0.3922,  ..., 0.4392, 0.4510, 0.4627],\n",
       "            ...,\n",
       "            [0.4078, 0.4039, 0.4667,  ..., 0.4235, 0.4196, 0.3961],\n",
       "            [0.3922, 0.4275, 0.4824,  ..., 0.3765, 0.4078, 0.3686],\n",
       "            [0.4000, 0.4353, 0.4902,  ..., 0.3922, 0.4078, 0.3882]]])],\n",
       "  [{'boxes': tensor([[333.,  36., 350.,  62.],\n",
       "            [324.,  41., 344.,  67.]]),\n",
       "    'labels': tensor([1, 1]),\n",
       "    'area': tensor([442., 520.]),\n",
       "    'iscrowd': tensor([0, 0])}])]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.25: 0.0, 0.5: 0.0, 0.75: 0.0, 0.9: 0.0, 'eval_time': 2.158421039581299}\n"
     ]
    }
   ],
   "source": [
    "metrics = evaluate(model, data_loader, device, thresh_list = [0.25, 0.5, 0.75, 0.9])\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(metrics: dict, epoch: int, thresh_list = [0.25, 0.5, 0.75, 0.9]) -> None:\n",
    "    print('[Epoch %-2.d] Evaluation results:' % (epoch + 1))\n",
    "    for thresh in thresh_list:\n",
    "        mAP = metrics[thresh]\n",
    "        print('    Threshold: %-3.3f | mAP: %-3.3f' % (thresh, mAP))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 ] Evaluation results:\n",
      "    Threshold: 0.250 | mAP: 0.000\n",
      "    Threshold: 0.500 | mAP: 0.000\n",
      "    Threshold: 0.750 | mAP: 0.000\n",
      "    Threshold: 0.900 | mAP: 0.000\n",
      "\n",
      "\n",
      "[Epoch 2 ] Evaluation results:\n",
      "    Threshold: 0.250 | mAP: 0.000\n",
      "    Threshold: 0.500 | mAP: 0.000\n",
      "    Threshold: 0.750 | mAP: 0.000\n",
      "    Threshold: 0.900 | mAP: 0.000\n",
      "\n",
      "\n",
      "[Epoch 3 ] Evaluation results:\n",
      "    Threshold: 0.250 | mAP: 0.000\n",
      "    Threshold: 0.500 | mAP: 0.000\n",
      "    Threshold: 0.750 | mAP: 0.000\n",
      "    Threshold: 0.900 | mAP: 0.000\n",
      "\n",
      "\n",
      "[Epoch 4 ] Evaluation results:\n",
      "    Threshold: 0.250 | mAP: 0.000\n",
      "    Threshold: 0.500 | mAP: 0.000\n",
      "    Threshold: 0.750 | mAP: 0.000\n",
      "    Threshold: 0.900 | mAP: 0.000\n",
      "\n",
      "\n",
      "[Epoch 5 ] Evaluation results:\n",
      "    Threshold: 0.250 | mAP: 0.000\n",
      "    Threshold: 0.500 | mAP: 0.000\n",
      "    Threshold: 0.750 | mAP: 0.000\n",
      "    Threshold: 0.900 | mAP: 0.000\n",
      "\n",
      "\n",
      "[Epoch 6 ] Evaluation results:\n",
      "    Threshold: 0.250 | mAP: 0.000\n",
      "    Threshold: 0.500 | mAP: 0.000\n",
      "    Threshold: 0.750 | mAP: 0.000\n",
      "    Threshold: 0.900 | mAP: 0.000\n",
      "\n",
      "\n",
      "[Epoch 7 ] Evaluation results:\n",
      "    Threshold: 0.250 | mAP: 0.000\n",
      "    Threshold: 0.500 | mAP: 0.000\n",
      "    Threshold: 0.750 | mAP: 0.000\n",
      "    Threshold: 0.900 | mAP: 0.000\n",
      "\n",
      "\n",
      "[Epoch 8 ] Evaluation results:\n",
      "    Threshold: 0.250 | mAP: 0.000\n",
      "    Threshold: 0.500 | mAP: 0.000\n",
      "    Threshold: 0.750 | mAP: 0.000\n",
      "    Threshold: 0.900 | mAP: 0.000\n",
      "\n",
      "\n",
      "[Epoch 9 ] Evaluation results:\n",
      "    Threshold: 0.250 | mAP: 0.000\n",
      "    Threshold: 0.500 | mAP: 0.000\n",
      "    Threshold: 0.750 | mAP: 0.000\n",
      "    Threshold: 0.900 | mAP: 0.000\n",
      "\n",
      "\n",
      "[Epoch 10] Evaluation results:\n",
      "    Threshold: 0.250 | mAP: 0.000\n",
      "    Threshold: 0.500 | mAP: 0.000\n",
      "    Threshold: 0.750 | mAP: 0.000\n",
      "    Threshold: 0.900 | mAP: 0.000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print_metrics(metrics, epoch=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, criterion, valid_loader):\n",
    "    #print(\"Calculating validation on hold-out....\")\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    accs = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in valid_loader:\n",
    "            inputs, labels = Variable(inputs).cuda(), Variable(labels).cuda()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            losses.append(loss.item())\n",
    "            acc = binary_acc(outputs, labels)\n",
    "            accs.append(acc.item())\n",
    "        \n",
    "    valid_loss = np.mean(losses)  # type: float\n",
    "    valid_acc = np.mean(accs)\n",
    "    #print('Average loss: %f' % valid_loss)\n",
    "    #print('Average accuracy: %f' % valid_acc)\n",
    "    \n",
    "    metrics = {'valid_loss': valid_loss, 'valid_acc': valid_acc}\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 5\n",
      "Valid Size: 4\n",
      "Starting Training...\n",
      "\n",
      "LOSS DICT:  {'loss_classifier': tensor(6.9498, grad_fn=<NllLossBackward>), 'loss_box_reg': tensor(0.0005, grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.6962, grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_rpn_box_reg': tensor(10.1094, grad_fn=<DivBackward0>)}\n",
      "[1,     1] Running Loss: 17.756\n",
      "           Number of Samples Seen: 2\n",
      "           Estimated Hours Remaining: 0.71\n",
      "\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"../vessel_detector_test_suite.py\", line 636, in <module>\n",
      "    main(savepath=None, backbone_state_dict=None)\n",
      "  File \"../vessel_detector_test_suite.py\", line 623, in main\n",
      "    batch_size=batch_size, print_every=print_every, num_epochs = num_epochs)\n",
      "  File \"../vessel_detector_test_suite.py\", line 364, in train_one_epoch\n",
      "    loss_dict = model(inputs, targets)\n",
      "  File \"/opt/miniconda3/envs/poisson/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/opt/miniconda3/envs/poisson/lib/python3.7/site-packages/torchvision/models/detection/generalized_rcnn.py\", line 96, in forward\n",
      "    features = self.backbone(images.tensors)\n",
      "  File \"/opt/miniconda3/envs/poisson/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/opt/miniconda3/envs/poisson/lib/python3.7/site-packages/torch/nn/modules/container.py\", line 117, in forward\n",
      "    input = module(input)\n",
      "  File \"/opt/miniconda3/envs/poisson/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/opt/miniconda3/envs/poisson/lib/python3.7/site-packages/torchvision/models/inception.py\", line 317, in forward\n",
      "    outputs = self._forward(x)\n",
      "  File \"/opt/miniconda3/envs/poisson/lib/python3.7/site-packages/torchvision/models/inception.py\", line 306, in _forward\n",
      "    branch7x7dbl = self.branch7x7dbl_3(branch7x7dbl)\n",
      "  File \"/opt/miniconda3/envs/poisson/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/opt/miniconda3/envs/poisson/lib/python3.7/site-packages/torchvision/models/inception.py\", line 440, in forward\n",
      "    x = self.bn(x)\n",
      "  File \"/opt/miniconda3/envs/poisson/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/opt/miniconda3/envs/poisson/lib/python3.7/site-packages/torch/nn/modules/batchnorm.py\", line 136, in forward\n",
      "    self.weight, self.bias, bn_training, exponential_average_factor, self.eps)\n",
      "  File \"/opt/miniconda3/envs/poisson/lib/python3.7/site-packages/torch/nn/functional.py\", line 2058, in batch_norm\n",
      "    training, momentum, eps, torch.backends.cudnn.enabled\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "! python ../vessel_detector_test_suite.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [2., 2., 2., 2.]])"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_1 = torch.Tensor([[0,0,0,0]])\n",
    "t_2 = torch.Tensor([[1,1,1,1], [2,2,2,2]])\n",
    "torch.vstack((t_1, t_2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
